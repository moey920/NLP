{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "간단한 seq2seq 만들기(Simple seq2seq).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOyWeaGBii+TVYH6xrysy2f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moey920/NLP/blob/master/seq2seq_%EB%A7%8C%EB%93%A4%EA%B8%B0(Simple_seq2seq).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7ymMxHr4u9K",
        "colab_type": "text"
      },
      "source": [
        "본 문서는 케라스를 이용해 RNN(Recurrent Neural Networks)모델인 Seq2Seq를 10분 안에 알려주는 튜토리얼 한글 버전입니다. Seq2Seq의 의미부터 케라스를 이용한 모델 구현을 다루고 있으며 본 문서 대상자는 recurrent networks와 keras에 대한 경험이 있다는 가정하에 진행합니다.\n",
        "\n",
        "Keras\n",
        "RNN\n",
        "LSTM\n",
        "NLP\n",
        "Seq2Seq\n",
        "GRU layer\n",
        "\n",
        "원문 : A ten-minute introduction to sequence-to-sequence learning in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGKFxG2z4rok",
        "colab_type": "text"
      },
      "source": [
        "# sequence-to-sequence 학습이란?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKwxWHNB6oq1",
        "colab_type": "text"
      },
      "source": [
        "sequence-to-sequence(Seq2Seq) 학습은 한 도메인(예: 영어 문장)에서 다른 도메인(예: 불어로 된 문장)으로 시퀀스(sequence)를 변환하는 모델 학습을 의미합니다.\n",
        "\n",
        "-  \"the cat sat on the mat\" -> [Seq2Seq model] -> \"le chat etait assis sur le tapis\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkcSScKq6uwV",
        "colab_type": "text"
      },
      "source": [
        "이 모델은 기계 번역 혹은 자유로운 질의응답에 사용됩니다. (자연어 질문을 주어 자연어 응답을 생성) –일반적으로, 텍스트를 생성해야 할 경우라면 언제든지 적용할 수 있습니다.\n",
        "\n",
        "해당 작업을 다루는 여러 가지 방법이(RNN 혹은 1D convnets) 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r8bGe_g69fh",
        "colab_type": "text"
      },
      "source": [
        "## 자명한(명확한) 사례 : 입력과 출력 시퀀스 길이가 같을 때"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk0tYv4U7Gle",
        "colab_type": "text"
      },
      "source": [
        "입력과 출력 시퀀스 길이가 같을 경우, 케라스 Long Short-Term Memory(LSTM)이나 GRU 계층(혹은 다수의 계층) 같은 모델들을 간단하게 구현할 수 있습니다. 예제 스크립트에선 어떻게 RNN으로 문자열로 인코딩된 숫자들에 대한 덧셈 연산을 학습할 수 있는지 보여주고 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVMzfb9b4vjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}