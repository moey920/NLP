{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GloVe Embedding.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1C64EOrA9xGcR1XhL5UG5vubtyWk5G0aK",
      "authorship_tag": "ABX9TyPoCaIWmxBaNvV+sfyXp4KU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moey920/NLP/blob/master/GloVe_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yTgl6AK6DYp",
        "colab_type": "text"
      },
      "source": [
        "## 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfupF3aIGUd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZE_2B4zdsMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jtNtvuTWyx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_file_link = '/content/drive/My Drive/datt/CNN_텍스트분류_정답_수정.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNpiSh2OW8p_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 불러온 데이터를 보면 id, document, label로 구분이 되어있습니다.\n",
        "train_data = pd.read_csv(train_file_link, header = 0, delimiter = '\\t', quoting = 3)\n",
        "train_data.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJezxR68XSrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUZsFcVKXMof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import json\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5K58Cy230V2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(review, okt, remove_stopwords = False, stop_words = []):\n",
        "    # 함수의 인자는 다음과 같다.\n",
        "    # review : 전처리할 텍스트\n",
        "    # okt : okt 객체를 반복적으로 생성하지 않고 미리 생성후 인자로 받는다.\n",
        "    # remove_stopword : 불용어를 제거할지 선택 기본값은 False\n",
        "    # stop_word : 불용어 사전은 사용자가 직접 입력해야함 기본값은 비어있는 리스트\n",
        "    \n",
        "    # 1. 한글 및 공백을 제외한 문자 모두 제거. + 영어 소문자, 대문자, 숫자도 제외\n",
        "    # 일단 OCR 결과의 원형을 학습시키기 위해 정규표현식을 사용하지 않고 학습시켜보겠습니다.\n",
        "    review_text = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9\\\\s]\", \" \",  review)\n",
        "    #review_text = re.sub(\" \", \"\",  review)\n",
        "    \n",
        "    # 2. okt 객체를 활용해서 형태소 단위로 나눈다.\n",
        "    word_review = okt.morphs(review_text, stem=True)\n",
        "    \n",
        "    if remove_stopwords:\n",
        "        \n",
        "        # 불용어 제거(선택적)\n",
        "        word_review = [token for token in word_review if not token in stop_words]\n",
        "        \n",
        "   \n",
        "    return word_review"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljaUk1G732Zi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = ['은', '는', '이', '가', '하', '아', '것', '들','의', '있', '되', '수', '보', \n",
        "              '주', '등', '한', '(', ')', '/', '*', '=', 'E', '|', '-', '.', ',', 'II', 'لالالالا', \n",
        "              '|||||||||', 'iii', '|||', '. ', '.', '\"', ' )', '[', ']', '\"']\n",
        "okt = Okt()\n",
        "clean_train_review = []\n",
        "\n",
        "for review in tqdm(train_data['document']):\n",
        "    # 비어있는 데이터에서 멈추지 않도록 string인 경우만 진행\n",
        "    if type(review) == str:\n",
        "        clean_train_review.append(preprocessing(review, okt, remove_stopwords = True, stop_words=stop_words))\n",
        "    else:\n",
        "        clean_train_review.append([])  #string이 아니면 비어있는 값 추가"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iR850QhGX5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = clean_train_review\n",
        "y_train = train_data['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8IWXCMYGddc",
        "colab_type": "text"
      },
      "source": [
        "문장과 레이블 데이터를 만들었습니다. 긍정인 문장은 레이블 1, 부정인 문장은 레이블이 0입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4W0_rzVGah_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = Tokenizer()\n",
        "t.fit_on_texts(sentences)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "\n",
        "print(vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkxAmboaGhpc",
        "colab_type": "text"
      },
      "source": [
        "케라스의 Tokenizer()를 사용하여 토큰화를 시켰습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UErURv0bGfKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_encoded = t.texts_to_sequences(sentences)\n",
        "print(X_encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xzE44uGGm8D",
        "colab_type": "text"
      },
      "source": [
        "각 문장에 대해서 정수 인코딩을 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4exF7-PoGjpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len=max(len(l) for l in X_encoded)\n",
        "print(max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNVs6oglGp7f",
        "colab_type": "text"
      },
      "source": [
        "문장 중에서 가장 길이가 긴 문장의 길이는 45입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-HitUKrGoo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train=pad_sequences(X_encoded, maxlen=max_len, padding='post')\n",
        "y_train=np.array(y_train)\n",
        "print(X_train)\n",
        "print(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuA494K-8Q5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data = X_train\n",
        "label_data = y_train\n",
        "word_index = t.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxezKibWGvOu",
        "colab_type": "text"
      },
      "source": [
        "모든 문장을 패딩하여 길이를 45로 만들어주었습니다. 훈련 데이터에 대한 전처리가 끝났습니다. 모델을 설계합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJMg_9GqkYfT",
        "colab_type": "text"
      },
      "source": [
        "### 훈련데이터 훈련,검증,테스트셋 나누기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoCrRetf7jAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST_SPLIT = 0.1\n",
        "RNG_SEED = 54648258\n",
        "VOCAB_SIZE = len(word_index)+1\n",
        "EMB_SIZE = 20\n",
        "BATCH_SIZE = 12\n",
        "NUM_EPOCHS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yta6xx8A8o0R",
        "colab_type": "text"
      },
      "source": [
        "인풋 데이터와 인풋 레이블을 앞서 설정한 변수값에 맞추어 훈련, 검증, 테스트 데이터로 나눠주도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNtjtHdC8z75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "input_train, input_test, label_train, label_test = train_test_split(input_data, label_data, test_size=TEST_SPLIT, random_state=RNG_SEED)\n",
        "#10%의 데이터를 테스트 데이터로 활용하도록 하겠습니다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDAOcjYf842T",
        "colab_type": "text"
      },
      "source": [
        "훈련(90%), 테스트(10%)를 나눴으니 훈련 데이터에서 일정량을 떼어 검증 데이터로 사용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P4camq98oQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_val = input_train[30000:]\n",
        "label_val = label_train[30000:]\n",
        "input_train = input_train[:30000]\n",
        "label_train = label_train[:30000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5QDHfau9DVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(input_train),len(label_train),': 훈련용 데이터/라벨','\\n',len(input_val),len(label_val),': 검증용 데이터/라벨','\\n',len(input_test),len(label_test),': 테스트용 데이터/라벨')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vj1JtHckded",
        "colab_type": "text"
      },
      "source": [
        "### 패딩작업, 원핫인코딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTGrimunAQb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from keras.layers import Flatten, Dropout\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxDrjCR39gs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터셋 전처리 : 문장 길이 맞추기\n",
        "input_train = sequence.pad_sequences(input_train, maxlen=max_len)\n",
        "input_val = sequence.pad_sequences(input_val, maxlen=max_len)\n",
        "input_test = sequence.pad_sequences(input_test, maxlen=max_len)\n",
        "\n",
        "#ndarray with shape(16000,120) : input_train\n",
        "#ndarray with shape(2000,120) : input_val\n",
        "#ndarray with shape(2000,120) : input_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NbSDVX6-XrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one-hot 인코딩\n",
        "label_train = np_utils.to_categorical(label_train)\n",
        "label_val = np_utils.to_categorical(label_val)\n",
        "label_test = np_utils.to_categorical(label_test)\n",
        "\n",
        "#ndarray with shape(16000, ) : label_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcWC3lB5HL5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9nHAyOMErED",
        "colab_type": "text"
      },
      "source": [
        "# 사전 훈련된 워드 임베딩(Pre-trained Word Embedding)\n",
        "이번엔 케라스의 임베딩 층(embedding layer)과 사전 훈련된 워드 임베딩(pre-trained word embedding)을 가져와서 사용하는 것을 비교해봅니다. 자연어 처리를 하려고 할 때 갖고 있는 훈련 데이터의 단어들을 임베딩 층(embedding layer)을 구현하여 임베딩 벡터로 학습하는 경우가 있습니다. 케라스에서는 이를 Embedding()이라는 도구를 사용하여 구현합니다.\n",
        "\n",
        "그런데 위키피디아 등과 같은 방대한 코퍼스를 가지고 Word2vec, FastText, GloVe 등을 통해서 이미 미리 훈련된 임베딩 벡터를 불러오는 방법을 사용하는 경우도 있습니다. 이는 현재 갖고 있는 훈련 데이터를 임베딩 층으로 처음부터 학습을 하는 방법과는 대조됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CnHOocFHO3f",
        "colab_type": "text"
      },
      "source": [
        "##  GloVe 사용하기\n",
        "\n",
        "이제 임베딩 층을 설계하기 위한 과정부터 달라집니다. 우선 다운로드 받은 파일인 glove.6B.zip의 압축을 풀면 그 안에 4개의 파일이 있는데 여기서 사용할 파일은 glove.6B.100d.txt 파일입니다. 해당 파일은 하나의 줄당 101개의 값을 가지는 리스트를 갖고 있습니다. 두 개의 줄만 읽어보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqBLJLNtLZd2",
        "colab_type": "text"
      },
      "source": [
        "[꿀팁]Colab에서 구글 드라이브 마운트를 통해 파일 가져오기\n",
        "Colab에서 구글 드라이브 API를 통해 굳이 연결하지 않더라도,\n",
        "Colab 왼쪽 상단을 보면 폴더 모양의 아이콘이 있습니다. 이를 클릭하고\n",
        "드라이브 마운트를 통해 계정에 연결해놓으면 개인 구글 드라이브에 있는 폴더들이 나타납니다.\n",
        "이 중 content/drive/Mydrive/ 로 들어가면 자신의 구글 드라이브에 있는 폴더와 파일들이 나타납니다.\n",
        "사용하고자 하는 파일을 우클릭하고 경로를 복사하면\n",
        "f = open('복사한 경로 붙여넣기')를 통해\n",
        "간편하게 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bq9DlfiKyVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n=0\n",
        "f = open('/content/drive/My Drive/datt/glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in f:\n",
        "    word_vector = line.split() # 각 줄을 읽어와서 word_vector에 저장.\n",
        "    print(word_vector) # 각 줄을 출력\n",
        "    word = word_vector[0] # word_vector에서 첫번째 값만 저장\n",
        "    print(word) # word_vector의 첫번째 값만 출력\n",
        "    n=n+1\n",
        "    if n==2:\n",
        "        break\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yOj8HwHK033",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(type(word_vector))\n",
        "print(len(word_vector))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXCiZuAwLx-k",
        "colab_type": "text"
      },
      "source": [
        "101개의 값 중에서 첫번째 값은 임베딩 벡터가 의미하는 단어를 의미하며, \n",
        "두번째 값부터 마지막 값은 해당 단어의 임베딩 벡터의 100개의 차원에서의 각 값을 의미합니다. \n",
        "\n",
        "즉, glove.6B.100d.txt는 수많은 단어에 대해서 100개의 차원을 가지는 임베딩 벡터로 제공하고 있습니다. \n",
        "\n",
        "위의 출력 결과는 단어 'the'에 대해서 100개의 차원을 가지는 임베딩 벡터와 단어 ','에 대해서 100개의 차원을 가지는 임베딩 벡터를 보여줍니다. \n",
        "\n",
        "그러면 이제 glove.6B.100d.txt에 있는 모든 임베딩 벡터들을 불러와보겠습니다. \n",
        "\n",
        "형식은 키(key)와 값(value)의 쌍(pair)를 가지는 파이썬의 사전형 구조를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBz513GXLtYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "embedding_dict = dict()\n",
        "f = open('/content/drive/My Drive/datt/glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in f:\n",
        "    word_vector = line.split()\n",
        "    word = word_vector[0]\n",
        "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32') # 100개의 값을 가지는 array로 변환\n",
        "    embedding_dict[word] = word_vector_arr\n",
        "f.close()\n",
        "print('%s개의 Embedding vector가 있습니다.' % len(embedding_dict))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV8j-EbJ20aX",
        "colab_type": "text"
      },
      "source": [
        "임의의 단어 'respectable'에 대해서 임베딩 벡터를 출력해봅니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r04XG8-p22UY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(embedding_dict['respectable'])\n",
        "print(len(embedding_dict['respectable']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyciEbBW24FQ",
        "colab_type": "text"
      },
      "source": [
        "벡터값이 출력되며 길이는 100인 것을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgitSMnb26hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "# 단어 집합 크기의 행과 100개의 열을 가지는 행렬 생성. 값은 전부 0으로 채워진다.\n",
        "np.shape(embedding_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95mZwpA_3ACa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(t.word_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpump3wk3ENV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word, i in t.word_index.items(): # 훈련 데이터의 단어 집합에서 단어를 1개씩 꺼내온다.\n",
        "    temp = embedding_dict.get(word) # 단어(key) 해당되는 임베딩 벡터의 100개의 값(value)를 임시 변수에 저장\n",
        "    if temp is not None:\n",
        "        embedding_matrix[i] = temp # 임수 변수의 값을 단어와 맵핑되는 인덱스의 행에 삽입"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIIIAT_Y2-Qp",
        "colab_type": "text"
      },
      "source": [
        "이제 훈련 데이터의 단어 집합의 모든 단어에 대해서 사전 훈련된 GloVe의 임베딩 벡터들을 맵핑하였습니다. 이제 이를 이용하여 임베딩 층(embedding layer)를 만들어보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL-uOnJR3LAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=True)\n",
        "#e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG7QKJ9_3N_P",
        "colab_type": "text"
      },
      "source": [
        "현재 실습에서 사전 훈련된 워드 임베딩을 100차원의 값인 것으로 사용하고 있기 때문에 임베딩 층의 output_dim의 인자값으로 100을 주어야 합니다. 그리고 사전 훈련된 워드 임베딩을 그대로 사용할 것이므로, 별도로 더 이상 훈련을 하지 않는다는 옵션을 줍니다. 이는 trainable=False로 선택할 수 있습니다.\n",
        "\n",
        "### 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42qyjTdv31Qo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 특정 클래스에 대한 정밀도\n",
        "def single_class_precision(interesting_class_id):\n",
        "    def prec(y_true, y_pred):\n",
        "        class_id_true = K.argmax(y_true, axis=-1) # y_true: 실제 값, 티아노 및 텐스플로우의 텐서(tensor)\n",
        "        class_id_pred = K.argmax(y_pred, axis=-1) # y_pred: 예측 값, 티아노 및 텐스플로우의 텐서(tensor)\n",
        "        precision_mask = K.cast(K.equal(class_id_pred, interesting_class_id), 'int32')\n",
        "        class_prec_tensor = K.cast(K.equal(class_id_true, class_id_pred), 'int32') * precision_mask\n",
        "        class_prec = K.cast(K.sum(class_prec_tensor), 'float32') / K.cast(K.maximum(K.sum(precision_mask), 1), 'float32')\n",
        "        return class_prec\n",
        "    return prec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHvAqE3P334V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 특정 클래스에 대한 재현율\n",
        "def single_class_recall(interesting_class_id):\n",
        "    def recall(y_true, y_pred):\n",
        "        class_id_true = K.argmax(y_true, axis=-1)\n",
        "        class_id_pred = K.argmax(y_pred, axis=-1)\n",
        "        recall_mask = K.cast(K.equal(class_id_true, interesting_class_id), 'int32')\n",
        "        class_recall_tensor = K.cast(K.equal(class_id_true, class_id_pred), 'int32') * recall_mask\n",
        "        class_recall = K.cast(K.sum(class_recall_tensor), 'float32') / K.cast(K.maximum(K.sum(recall_mask), 1), 'float32')\n",
        "        return class_recall\n",
        "    return recall"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOIACxBv-rBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 256, weights=[embedding_matrix], input_length=max_len, trainable=True)) # 오류가 난다면 차원을 변경하세요.\n",
        "#model.add(Flatten()) # Dense의 입력으로 넣기위함.\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(256,\n",
        "                 3,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(20, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c6arsCL37dw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(e) \n",
        "model.add(Flatten()) # Dense의 입력으로 넣기위함.\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(20, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nne3qidq41aM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eopW2WPv3QIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
        "              metrics=['accuracy',\n",
        "                       single_class_precision(0), single_class_recall(0),\n",
        "                       single_class_precision(1), single_class_recall(1),\n",
        "                       single_class_precision(2), single_class_recall(2),\n",
        "                       single_class_precision(3), single_class_recall(3),\n",
        "                       single_class_precision(4), single_class_recall(4),\n",
        "                       single_class_precision(5), single_class_recall(5),\n",
        "                       single_class_precision(6), single_class_recall(6),\n",
        "                       single_class_precision(7), single_class_recall(7),\n",
        "                       single_class_precision(8), single_class_recall(8),\n",
        "                       single_class_precision(9), single_class_recall(9),\n",
        "                       single_class_precision(10), single_class_recall(10),\n",
        "                       single_class_precision(11), single_class_recall(11),\n",
        "                       single_class_precision(12), single_class_recall(12),\n",
        "                       single_class_precision(13), single_class_recall(13),\n",
        "                       single_class_precision(14), single_class_recall(14),\n",
        "                       single_class_precision(15), single_class_recall(15),\n",
        "                       single_class_precision(16), single_class_recall(16),\n",
        "                       single_class_precision(17), single_class_recall(17),\n",
        "                       single_class_precision(18), single_class_recall(18),\n",
        "                       single_class_precision(19), single_class_recall(19)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZKtIsNe842a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(input_train, label_train, epochs=15, verbose=1, validation_data=(input_val, label_val)) # validation_data는 각 훈련마다 결과값을 도출할 측정 데이터를 의미한다. 따라서 가중치 업데이트는 되지 않는다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6MzK10Z-8KW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6. 모델 평가하기\n",
        "loss_and_metrics = model.evaluate(input_test, label_test, batch_size=256)\n",
        "print('## evaluation loss and_metrics ##')\n",
        "print(loss_and_metrics)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_wV2JPe_TYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.predict_classes(input_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6b6Okgc_cPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data['label'] = model.predict_classes(input_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IlsBpyc_f6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data[25010:25020]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olWvGms4_jYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install xlsxwriter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIWlwHlA_lGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataframe = pd.DataFrame(train_data)\n",
        "dataframe.to_excel(\"/content/drive/My Drive/text/GloVeEmbedding_result(1024).xlsx\", engine='xlsxwriter', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juB41Mq6Bce-",
        "colab_type": "text"
      },
      "source": [
        "### 결과 보고\n",
        "\n",
        "loss_and_metrics = model.evaluate(input_test, label_test, batch_size=256)\n",
        "- 모델 평가에서의 배치사이즈를 256, 512로 설정하여 정답레이블-예측레이블 오차를 측장하여 비교해봤으나, 둘 다 39371개 중 810를 잘못 예측하였습니다. \n",
        "- 배치 사이즈가 1일 경우, 128일 경우 정밀도와 재현율이 현저하게 떨어졌습니다.\n",
        "- 256, 512의 경우 완벽하게 정확하게 예측이 일치하는 것으로 보아, 과대적합이 발생하여 훈련데이터에 적응해버린 것이 아닌가 추측됩니다. \n",
        "-- 잘못 생각한 것이었습니다. 평가층에서 배치 사이즈를 변경한다고하여, 이미 훈련된 모델로 predict할 땐 전혀 영향을 끼치지 않습니다. 단치 배치 사이즈에 따른 모델을 평가하는 인자인 것 같습니다. \n",
        "-- 따라서 평가에서의 배치 사이즈가 아닌 glove 임베딩 층의 차원을 변경하여 어떤 결과가 나오는지 확인해보곘습니다.\n",
        "- 일반적인 자체 데이터셋으로 임베딩층을 설정했을 때와, Pre-Trained GloVe 임베딩 층을 사용했을 때 결과가 별반 다르지 않았습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni0D_zRYCtKx",
        "colab_type": "text"
      },
      "source": [
        "### 추가 검사 방안\n",
        "위의 코드는 glove.100d 파일을 이용하여 100차원의 임베딩 층을 생성하였습니다.\n",
        "glove.300d 파일을 이용하여 300차원의 임베딩 층을 사용하여 차이가 있는지 확인해보곘습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6mtY9gDCRa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "embedding_dict = dict()\n",
        "f = open('/content/drive/My Drive/datt/glove.6B.300d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in f:\n",
        "    word_vector = line.split()\n",
        "    word = word_vector[0]\n",
        "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32') # 300개의 값을 가지는 array로 변환\n",
        "    embedding_dict[word] = word_vector_arr\n",
        "f.close()\n",
        "print('%s개의 Embedding vector가 있습니다.' % len(embedding_dict))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xoy-pLUDmdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(embedding_dict['respectable'])\n",
        "print(len(embedding_dict['respectable']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7TcbNUgDpdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "# 단어 집합 크기의 행과 300개의 열을 가지는 행렬 생성. 값은 전부 0으로 채워진다.\n",
        "np.shape(embedding_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrMDTuR8Dspk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(t.word_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_q8IjRxDvG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word, i in t.word_index.items(): # 훈련 데이터의 단어 집합에서 단어를 1개씩 꺼내온다.\n",
        "    temp = embedding_dict.get(word) # 단어(key) 해당되는 임베딩 벡터의 300개의 값(value)를 임시 변수에 저장\n",
        "    if temp is not None:\n",
        "        embedding_matrix[i] = temp # 임수 변수의 값을 단어와 맵핑되는 인덱스의 행에 삽입"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hVgeEh9Dzu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 특정 클래스에 대한 정밀도\n",
        "def single_class_precision(interesting_class_id):\n",
        "    def prec(y_true, y_pred):\n",
        "        class_id_true = K.argmax(y_true, axis=-1) # y_true: 실제 값, 티아노 및 텐스플로우의 텐서(tensor)\n",
        "        class_id_pred = K.argmax(y_pred, axis=-1) # y_pred: 예측 값, 티아노 및 텐스플로우의 텐서(tensor)\n",
        "        precision_mask = K.cast(K.equal(class_id_pred, interesting_class_id), 'int32')\n",
        "        class_prec_tensor = K.cast(K.equal(class_id_true, class_id_pred), 'int32') * precision_mask\n",
        "        class_prec = K.cast(K.sum(class_prec_tensor), 'float32') / K.cast(K.maximum(K.sum(precision_mask), 1), 'float32')\n",
        "        return class_prec\n",
        "    return prec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNuBjgWRD2lt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 특정 클래스에 대한 재현율\n",
        "def single_class_recall(interesting_class_id):\n",
        "    def recall(y_true, y_pred):\n",
        "        class_id_true = K.argmax(y_true, axis=-1)\n",
        "        class_id_pred = K.argmax(y_pred, axis=-1)\n",
        "        recall_mask = K.cast(K.equal(class_id_true, interesting_class_id), 'int32')\n",
        "        class_recall_tensor = K.cast(K.equal(class_id_true, class_id_pred), 'int32') * recall_mask\n",
        "        class_recall = K.cast(K.sum(class_recall_tensor), 'float32') / K.cast(K.maximum(K.sum(recall_mask), 1), 'float32')\n",
        "        return class_recall\n",
        "    return recall"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpSg-7heD3MK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_uFK6DeGkiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from keras.layers import Flatten, Dropout\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XxorF60D-d2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_len, trainable=True)) # 배치사이즈가 기존(128)보다 2배 커지면 어떻게 될까? => \n",
        "#model.add(Flatten()) # Dense의 입력으로 넣기위함.\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(256,\n",
        "                 3,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(20, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAReq18REB_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
        "              metrics=['accuracy',\n",
        "                       single_class_precision(0), single_class_recall(0),\n",
        "                       single_class_precision(1), single_class_recall(1),\n",
        "                       single_class_precision(2), single_class_recall(2),\n",
        "                       single_class_precision(3), single_class_recall(3),\n",
        "                       single_class_precision(4), single_class_recall(4),\n",
        "                       single_class_precision(5), single_class_recall(5),\n",
        "                       single_class_precision(6), single_class_recall(6),\n",
        "                       single_class_precision(7), single_class_recall(7),\n",
        "                       single_class_precision(8), single_class_recall(8),\n",
        "                       single_class_precision(9), single_class_recall(9),\n",
        "                       single_class_precision(10), single_class_recall(10),\n",
        "                       single_class_precision(11), single_class_recall(11),\n",
        "                       single_class_precision(12), single_class_recall(12),\n",
        "                       single_class_precision(13), single_class_recall(13),\n",
        "                       single_class_precision(14), single_class_recall(14),\n",
        "                       single_class_precision(15), single_class_recall(15),\n",
        "                       single_class_precision(16), single_class_recall(16),\n",
        "                       single_class_precision(17), single_class_recall(17),\n",
        "                       single_class_precision(18), single_class_recall(18),\n",
        "                       single_class_precision(19), single_class_recall(19)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0AHOsfzEF-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(input_train, label_train, epochs=15, verbose=1, validation_data=(input_val, label_val)) # validation_data는 각 훈련마다 결과값을 도출할 측정 데이터를 의미한다. 따라서 가중치 업데이트는 되지 않는다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgoqWVTgEKP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6. 모델 평가하기\n",
        "loss_and_metrics = model.evaluate(input_test, label_test, batch_size=512)\n",
        "print('## evaluation loss and_metrics ##')\n",
        "print(loss_and_metrics)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYmqrOXuEMqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.predict_classes(input_data)\n",
        "train_data['label'] = model.predict_classes(input_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HrzWGfRETiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataframe = pd.DataFrame(train_data)\n",
        "dataframe.to_excel(\"/content/drive/My Drive/text/GloVeEmbedding_result(glove300d).xlsx\", engine='xlsxwriter', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrS6IIxST9ry",
        "colab_type": "text"
      },
      "source": [
        "에측값을 label로 삽입한 엑셀 파일을 다운받아, 가지고 잇던 정답 레이블과 비교해보니 39370개 데이터 중 541개 데이터(문장) 예측에 실패했습니다. \n",
        "무려 98.65586%의 예측 정확도를 가지고 있습니다!\n",
        "- 이미 정상적으로 각 레이블에 적합하도록 사전작업을 거친 데이터로 확인한 결과라, OCR 결과 텍스트를 그대로 사용할 경우의 정확도라고 말할 수 없습니다."
      ]
    }
  ]
}