{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "receipt_transformer_pytorch_kobert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moey920/NLP/blob/master/receipt_transformer_pytorch_kobert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7SA0Bln7IB9",
        "colab_type": "code",
        "outputId": "55b7eaa7-c87e-40c1-93fc-9227ce63420e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        }
      },
      "source": [
        "!pip install mxnet-cu101 #\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece==0.1.85\n",
        "!pip install transformers==2.1.1\n",
        "!pip install torch==1.3.1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mxnet-cu101 in /usr/local/lib/python3.6/dist-packages (1.6.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101) (0.8.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101) (2.23.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101) (1.18.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (2.9)\n",
            "Requirement already satisfied: gluonnlp in /usr/local/lib/python3.6/dist-packages (0.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (1.18.4)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (0.29.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (20.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp) (1.12.0)\n",
            "Requirement already satisfied: sentencepiece==0.1.85 in /usr/local/lib/python3.6/dist-packages (0.1.85)\n",
            "Requirement already satisfied: transformers==2.1.1 in /usr/local/lib/python3.6/dist-packages (2.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (0.1.85)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (0.0.43)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (1.13.13)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (1.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (4.41.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (2019.12.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.1.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.1.1) (0.15.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.1.1) (1.12.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.1.1) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.1.1) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.13 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.1.1) (1.16.13)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->transformers==2.1.1) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->transformers==2.1.1) (2.8.1)\n",
            "Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.3.1) (1.18.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcCygf-M7ICD",
        "colab_type": "code",
        "outputId": "f1d0538d-4e52-4c50-8ea1-bfe81464d352",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-6ljd8rnu\n",
            "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-6ljd8rnu\n",
            "Requirement already satisfied (use --upgrade to upgrade): kobert==0.1.1 from git+https://****@github.com/SKTBrain/KoBERT.git@master in /usr/local/lib/python3.6/dist-packages\n",
            "Building wheels for collected packages: kobert\n",
            "  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert: filename=kobert-0.1.1-cp36-none-any.whl size=12871 sha256=48e12f69ac64f6228de87017a0505268fdd3fced6801758d2c389b706159c3a5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4_op5f_w/wheels/a2/b0/41/435ee4e918f91918be41529283c5ff86cd010f02e7525aecf3\n",
            "Successfully built kobert\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq1q2U_97ICI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT4AwgmH7ICN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SAl2rbE7ICW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AdamW\n",
        "from transformers.optimization import WarmupLinearSchedule"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-TsIkbXNy4W",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBQq2yAf7ICZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##GPU 사용 시\n",
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkrWX0IQ7ICn",
        "colab_type": "code",
        "outputId": "33e80d39-fbae-4c55-fb49-3861a858d445",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "bertmodel, vocab = get_pytorch_kobert_model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n",
            "using cached model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMmr-G3A7ICr",
        "colab_type": "code",
        "outputId": "81827bdf-ca82-4880-e4df-9bb854e4b667",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "'''\n",
        "!wget https://www.dropbox.com/s/374ftkec978br3d/ratings_train.txt?dl=1\n",
        "!wget https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n!wget https://www.dropbox.com/s/374ftkec978br3d/ratings_train.txt?dl=1\\n!wget https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H3Rof9UvhPp",
        "colab_type": "code",
        "outputId": "b5178b6c-1d9f-4627-c619-1a8c0689ab39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZexzBWVD7ICy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_train = nlp.data.TSVDataset(\"/content/drive/My Drive/캐시카우_노하람인턴_공유폴더/KoBERT/dataset/ratings_train.txt\", field_indices=[1,2], num_discard_samples=1)\n",
        "dataset_test = nlp.data.TSVDataset(\"/content/drive/My Drive/캐시카우_노하람인턴_공유폴더/KoBERT/dataset/ratings_test.txt\", field_indices=[1,2], num_discard_samples=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjGLYfrMCvZU",
        "colab_type": "code",
        "outputId": "eb10b740-8530-49dd-ccce-5f09d1d92fcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(dataset_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['CJ올리브영( ㈜ 이선신보집', 'CJ올리브영㈜ 인천신포점']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfOfOqgW7IC4",
        "colab_type": "code",
        "outputId": "82c19710-2d90-4f85-f516-f54ca657a642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE4qYp777IC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [transform([i[label_idx]]) for i in dataset]\n",
        "        #self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return (len(self.labels))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJtetpVY7IDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Setting parameters\n",
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdiV5Edn7IDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
        "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt7DnXyrR0Yq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.utils.data as data\n",
        "\n",
        "def my_collate(batch):\n",
        "    #data = torch.stack([item[0].unsqueeze(0) for item in batch], 0)\n",
        "    data = torch.Tensor([item[0] for item in batch], 0)\n",
        "    target = torch.Tensor([item[1] for item in batch])\n",
        "    return [data, target]\n",
        "\n",
        "class dataset(data.Dataset):\n",
        "    def __init__(self):\n",
        "        super(dataset, self).__init__()\n",
        "\n",
        "    def __len__(self):\n",
        "        return 100\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return torch.rand(5, 6), list(range(index))\n",
        "\n",
        "dataloader = data.DataLoader(dataset=data_train,\n",
        "                      batch_size=batch_size,\n",
        "                      shuffle=True,\n",
        "                      collate_fn=my_collate, # use custom collate function here\n",
        "                      pin_memory=True)\n",
        "\n",
        "for instance in dataloader:\n",
        "    print(instance[0].shape, len(instance[1]))\n",
        "    for labels in instance[1]:\n",
        "        print('length', len(labels))\n",
        "    raw_input()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zJMlwIuVLaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.utils.data as data\n",
        "\n",
        "def my_collate(batch):\n",
        "    data = torch.stack([item[0].unsqueeze(0) for item in batch], 0)\n",
        "    target = torch.Tensor([item[1] for item in batch])\n",
        "    return [data, target]\n",
        "\n",
        "class dataset(data.Dataset):\n",
        "    def __init__(self):\n",
        "        super(dataset, self).__init__()\n",
        "\n",
        "    def __len__(self):\n",
        "        return 100\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return torch.rand(5, 6), list(range(index))\n",
        "\n",
        "dataloader = data.DataLoader(dataset=data_train,\n",
        "                      batch_size=batch_size,\n",
        "                      shuffle=True,\n",
        "                      collate_fn=my_collate, # use custom collate function here\n",
        "                      pin_memory=True)\n",
        "\n",
        "for instance in dataloader:\n",
        "    print(instance[0].shape, len(instance[1]))\n",
        "    for labels in instance[1]:\n",
        "        print('length', len(labels))\n",
        "    raw_input()\n",
        "    '''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2BWj33m7IDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2fKFQVa20xY",
        "colab_type": "code",
        "outputId": "4f494c68-9e55-41e6-bbff-60c556605b16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(type(test_dataloader))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.utils.data.dataloader.DataLoader'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKUyX_PC7IDn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=20,\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asygfNvE7IDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BERTClassifier(bertmodel,   dr_rate=0.5).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcGmkSzy7IDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4avbl_L7IDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arlLv7tM7ID4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A7j0kvv7ID7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_step, t_total=t_total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpTEuLgM7ID-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8j2TNl5eNN4m",
        "colab_type": "code",
        "outputId": "0791509b-fa5a-459d-cc1d-7c5d0332e8ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "source": [
        "print(label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([[   2,  643, 6973,  ...,    1,    1,    1],\n",
            "        [   2, 3790, 5483,  ...,    1,    1,    1],\n",
            "        [   2, 1316, 7993,  ...,    1,    1,    1],\n",
            "        ...,\n",
            "        [   2,  702, 4998,  ...,    1,    1,    1],\n",
            "        [   2,  524,  524,  ...,    1,    1,    1],\n",
            "        [   2,  717, 3025,  ...,    1,    1,    1]], dtype=torch.int32), tensor([14, 10,  9,  8, 26, 18, 20, 16, 16,  3, 19, 19, 12,  5, 16, 16, 11, 10,\n",
            "        33, 11, 15, 10, 18, 18,  5,  6,  5,  3,  4, 10, 22, 12, 12, 13, 16, 11,\n",
            "        11, 14, 25,  9, 18, 16, 14, 24,  6,  8,  8, 12, 13, 19,  7, 13, 49,  6,\n",
            "        13,  4,  6,  5,  5,  4,  6,  9, 49,  4], dtype=torch.int32), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fclpq8gn7IEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    model.train()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "    model.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        test_acc += calc_accuracy(out, label)\n",
        "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnkBKlpq5um2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(segment_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "su6d20dn6I-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PouYk-TL6Pmn",
        "colab_type": "code",
        "outputId": "bb6109c9-1138-4aea-fcba-f69f6a585ab3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWBPC0tV6Rr9",
        "colab_type": "code",
        "outputId": "83315f98-f215-411f-f5ff-4398262b0bdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "source": [
        "print(label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([[   2,  643, 6973,  ...,    1,    1,    1],\n",
            "        [   2, 3790, 5483,  ...,    1,    1,    1],\n",
            "        [   2, 1316, 7993,  ...,    1,    1,    1],\n",
            "        ...,\n",
            "        [   2,  702, 4998,  ...,    1,    1,    1],\n",
            "        [   2,  524,  524,  ...,    1,    1,    1],\n",
            "        [   2,  717, 3025,  ...,    1,    1,    1]], dtype=torch.int32), tensor([14, 10,  9,  8, 26, 18, 20, 16, 16,  3, 19, 19, 12,  5, 16, 16, 11, 10,\n",
            "        33, 11, 15, 10, 18, 18,  5,  6,  5,  3,  4, 10, 22, 12, 12, 13, 16, 11,\n",
            "        11, 14, 25,  9, 18, 16, 14, 24,  6,  8,  8, 12, 13, 19,  7, 13, 49,  6,\n",
            "        13,  4,  6,  5,  5,  4,  6,  9, 49,  4], dtype=torch.int32), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yM4SAyQ6T_z",
        "colab_type": "code",
        "outputId": "0138da5f-d263-4f34-cb02-e4d4bb77d2c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        }
      },
      "source": [
        "torch.stack(label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-105-f5128de79d1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Tensors must have same number of dimensions: got 3 and 2 at /pytorch/aten/src/TH/generic/THTensor.cpp:680"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meNNEDaJ60yJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}