{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT(Bidirectional Encoder Representations from Transformers) 구현(기준).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNElL3BegIZsieSTke4bFkR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moey920/NLP/blob/master/BERT(Bidirectional_Encoder_Representations_from_Transformers)_%EA%B5%AC%ED%98%84(%EA%B8%B0%EC%A4%80).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNIMH6uJOI4a",
        "colab_type": "text"
      },
      "source": [
        "BERT Transformer Encoder를 활용한 Pretrained LM(Langauge Model)입니다.\n",
        "\n",
        "- Pretrained LM이란 레이블이 없는 많은 데이터를 비지도 학습 방법으로 학습을 해서 모델이 언어를 이해 할 수 있도록 한 후 특정 Task에 적용해서 좋은 성능을 내는 방법을 의미 합니다.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4UlKXo3UXE7",
        "colab_type": "text"
      },
      "source": [
        "# 0. Base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxVMRMl3AppX",
        "colab_type": "text"
      },
      "source": [
        "## Pip Install\n",
        "필요한 패키지를 pip를 이용해서 설치합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJgMQwecAvQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install sentencepiece\n",
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34in6MYtAvvQ",
        "colab_type": "text"
      },
      "source": [
        "## Google Drive Mount\n",
        "\n",
        "Google Colab 제일 왼쪽의 폴더 버튼을 클릭해서 구글 드라이브에 연결해주세요.\n",
        "\n",
        "혹은 아래의 코드를 실행하여 주세요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAg4v_nOA_4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "### data를 저장할 폴더 입니다. 환경에 맞게 수정 하세요.\n",
        "data_dir = \"/content/drive/My Drive/Data/transformer-evolution\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51wQ5n7HCr8d",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VsulW89Cuzm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "from random import random, randrange, randint, shuffle, choice\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm, tqdm_notebook, trange\n",
        "import sentencepiece as spm\n",
        "import wget\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6BxB2_WCxaH",
        "colab_type": "text"
      },
      "source": [
        "폴더의 목록을 확인\n",
        "\n",
        "Google Drive mount가 잘 되었는지 확인하기 위해 data_dir 목록을 확인 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xhr06linC1nc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for f in os.listdir(data_dir):\n",
        "  print(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv1d9u5yC3W6",
        "colab_type": "text"
      },
      "source": [
        "## Vocab 및 입력\n",
        "\n",
        "Sentencepiece를 활용해 Vocab 만들기를 통해 만들어 놓은 vocab을 로딩 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6NJniQbC7QB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vocab loading\n",
        "vocab_file = f\"{data_dir}/kowiki.model\"\n",
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.load(vocab_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrUWbrVzC9ZD",
        "colab_type": "text"
      },
      "source": [
        "## Config\n",
        "\n",
        "모델에 설정 값을 전달하기 위한 config를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ptcjPzKDAnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" configuration json을 읽어들이는 class \"\"\"\n",
        "class Config(dict): \n",
        "    __getattr__ = dict.__getitem__\n",
        "    __setattr__ = dict.__setitem__\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, file):\n",
        "        with open(file, 'r') as f:\n",
        "            config = json.loads(f.read())\n",
        "            return Config(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXISg1_4DGtj",
        "colab_type": "text"
      },
      "source": [
        "## Common Class\n",
        "\n",
        "공통으로 사용되는 Class 및 함수 입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trdiYKDzDKki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" sinusoid position encoding \"\"\"\n",
        "def get_sinusoid_encoding_table(n_seq, d_hidn):\n",
        "    def cal_angle(position, i_hidn):\n",
        "        return position / np.power(10000, 2 * (i_hidn // 2) / d_hidn)\n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, i_hidn) for i_hidn in range(d_hidn)]\n",
        "\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # even index sin \n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # odd index cos\n",
        "\n",
        "    return sinusoid_table\n",
        "\n",
        "\n",
        "\"\"\" attention pad mask \"\"\"\n",
        "def get_attn_pad_mask(seq_q, seq_k, i_pad):\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    pad_attn_mask = seq_k.data.eq(i_pad).unsqueeze(1).expand(batch_size, len_q, len_k)  # <pad>\n",
        "    return pad_attn_mask\n",
        "\n",
        "\n",
        "\"\"\" attention decoder mask \"\"\"\n",
        "def get_attn_decoder_mask(seq):\n",
        "    subsequent_mask = torch.ones_like(seq).unsqueeze(-1).expand(seq.size(0), seq.size(1), seq.size(1))\n",
        "    subsequent_mask = subsequent_mask.triu(diagonal=1) # upper triangular part of a matrix(2-D)\n",
        "    return subsequent_mask\n",
        "\n",
        "\n",
        "\"\"\" scale dot product attention \"\"\"\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.scale = 1 / (self.config.d_head ** 0.5)\n",
        "    \n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # (bs, n_head, n_q_seq, n_k_seq)\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)).mul_(self.scale)\n",
        "        scores.masked_fill_(attn_mask, -1e9)\n",
        "        # (bs, n_head, n_q_seq, n_k_seq)\n",
        "        attn_prob = nn.Softmax(dim=-1)(scores)\n",
        "        attn_prob = self.dropout(attn_prob)\n",
        "        # (bs, n_head, n_q_seq, d_v)\n",
        "        context = torch.matmul(attn_prob, V)\n",
        "        # (bs, n_head, n_q_seq, d_v), (bs, n_head, n_q_seq, n_v_seq)\n",
        "        return context, attn_prob\n",
        "\n",
        "\n",
        "\"\"\" multi head attention \"\"\"\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.W_Q = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
        "        self.W_K = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
        "        self.W_V = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
        "        self.scaled_dot_attn = ScaledDotProductAttention(self.config)\n",
        "        self.linear = nn.Linear(self.config.n_head * self.config.d_head, self.config.d_hidn)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "    \n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        batch_size = Q.size(0)\n",
        "        # (bs, n_head, n_q_seq, d_head)\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
        "        # (bs, n_head, n_k_seq, d_head)\n",
        "        k_s = self.W_K(K).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
        "        # (bs, n_head, n_v_seq, d_head)\n",
        "        v_s = self.W_V(V).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
        "\n",
        "        # (bs, n_head, n_q_seq, n_k_seq)\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.config.n_head, 1, 1)\n",
        "\n",
        "        # (bs, n_head, n_q_seq, d_head), (bs, n_head, n_q_seq, n_k_seq)\n",
        "        context, attn_prob = self.scaled_dot_attn(q_s, k_s, v_s, attn_mask)\n",
        "        # (bs, n_head, n_q_seq, h_head * d_head)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.config.n_head * self.config.d_head)\n",
        "        # (bs, n_head, n_q_seq, e_embd)\n",
        "        output = self.linear(context)\n",
        "        output = self.dropout(output)\n",
        "        # (bs, n_q_seq, d_hidn), (bs, n_head, n_q_seq, n_k_seq)\n",
        "        return output, attn_prob\n",
        "\n",
        "\n",
        "\"\"\" feed forward \"\"\"\n",
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=self.config.d_hidn, out_channels=self.config.d_ff, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=self.config.d_ff, out_channels=self.config.d_hidn, kernel_size=1)\n",
        "        self.active = F.gelu\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # (bs, d_ff, n_seq)\n",
        "        output = self.active(self.conv1(inputs.transpose(1, 2)))\n",
        "        # (bs, n_seq, d_hidn)\n",
        "        output = self.conv2(output).transpose(1, 2)\n",
        "        output = self.dropout(output)\n",
        "        # (bs, n_seq, d_hidn)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQBsTWCCOJ8W",
        "colab_type": "text"
      },
      "source": [
        "# 1. Config\n",
        "Transformer와 파라미터를 동일하게 설정 했습니다.\n",
        "BERT는 Encoder만 사용하므로 항목 중 Decoder 부분은 제거 했습니다.\n",
        "BERT Encoder는 기본 입력에 추가로 Segment 정보를 입력 받는데 Segment개수를 정의하는 n_seg_type을 추가로 정의 했습니다.\n",
        "기본 파라미터는 config.json을 참고 하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYOFbqZ6N3mK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = Config({\n",
        "    \"n_enc_vocab\": len(vocab),\n",
        "    \"n_enc_seq\": 256,\n",
        "    \"n_seg_type\": 2,\n",
        "    \"n_layer\": 6,\n",
        "    \"d_hidn\": 256,\n",
        "    \"i_pad\": 0,\n",
        "    \"d_ff\": 1024,\n",
        "    \"n_head\": 4,\n",
        "    \"d_head\": 64,\n",
        "    \"dropout\": 0.1,\n",
        "    \"layer_norm_epsilon\": 1e-12\n",
        "})\n",
        "print(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awLimRnTOc65",
        "colab_type": "text"
      },
      "source": [
        "# 2. Encoder\n",
        "\n",
        "BERT는 표준 Transformer의 Decoder는 사용하지 않고 Encoder만 사용합니다.\n",
        "\n",
        "Position Embedding 학습, Segment Embedding 추가 두가지 이외에 나머지 부분은 표준 Transformer와 동일합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xaTFv6KOhrU",
        "colab_type": "text"
      },
      "source": [
        "## Encoder Layer\n",
        "\n",
        "표준 Transformer EncoderLayer와 동일합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVr6nW7fOU_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" encoder layer \"\"\"\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(self.config)\n",
        "        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
        "        self.pos_ffn = PoswiseFeedForwardNet(self.config)\n",
        "        self.layer_norm2 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
        "    \n",
        "    def forward(self, inputs, attn_mask):\n",
        "        # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
        "        att_outputs, attn_prob = self.self_attn(inputs, inputs, inputs, attn_mask)\n",
        "        att_outputs = self.layer_norm1(inputs + att_outputs)\n",
        "        # (bs, n_enc_seq, d_hidn)\n",
        "        ffn_outputs = self.pos_ffn(att_outputs)\n",
        "        ffn_outputs = self.layer_norm2(ffn_outputs + att_outputs)\n",
        "        # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
        "        return ffn_outputs, attn_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VOyCvINOm9u",
        "colab_type": "text"
      },
      "source": [
        "표준 Transformer Encoder와 아래 내용이 다릅니다.\n",
        "\n",
        "- 1. Position을 학습할 수 있도록 하였습니다. (줄: 8)\n",
        "    \n",
        "    표준 Transformer에서는 sinusoid encoding을 구한 후 이 값으로 position embedding을 초기화 할 때 freeze 옵션을 True로 주어 position embedding이 학습되지 않도록 하였습니다.\n",
        "BERT는 position embedding을 기본 값으로 초기화 한 후 학습되도록 하였습니다.\n",
        "- 2. Segment Embedding을 추가 했습니다. (줄: 9)\n",
        "\n",
        "    Segment Embedding은 위 그림과 같이 두개의 문장이 들어 갈 경우 첫 번째 문장과 두 번째 문장을 구분하기 위해 사용 됩니다.\n",
        "- 3. Encoder input에 Segment 정보를 추가 했습니다. (줄: 13)\n",
        "\n",
        "- 4. Token, Position 및 Segment 3가지 Embedding을 더 합니다. (줄: 19)\n",
        "\n",
        "    표준 Transformer에서는 Token, Position 2가지 Embedding을 더 합니다.\n",
        "나머지는 Transformer Encoder와 동일합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbHLQlMNO5dL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" encoder \"\"\"\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.enc_emb = nn.Embedding(self.config.n_enc_vocab, self.config.d_hidn)\n",
        "        self.pos_emb = nn.Embedding(self.config.n_enc_seq + 1, self.config.d_hidn)\n",
        "        self.seg_emb = nn.Embedding(self.config.n_seg_type, self.config.d_hidn)\n",
        "\n",
        "        self.layers = nn.ModuleList([EncoderLayer(self.config) for _ in range(self.config.n_layer)])\n",
        "    \n",
        "    def forward(self, inputs, segments):\n",
        "        positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).expand(inputs.size(0), inputs.size(1)).contiguous() + 1\n",
        "        pos_mask = inputs.eq(self.config.i_pad)\n",
        "        positions.masked_fill_(pos_mask, 0)\n",
        "\n",
        "        # (bs, n_enc_seq, d_hidn)\n",
        "        outputs = self.enc_emb(inputs) + self.pos_emb(positions)  + self.seg_emb(segments)\n",
        "\n",
        "        # (bs, n_enc_seq, n_enc_seq)\n",
        "        attn_mask = get_attn_pad_mask(inputs, inputs, self.config.i_pad)\n",
        "\n",
        "        attn_probs = []\n",
        "        for layer in self.layers:\n",
        "            # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
        "            outputs, attn_prob = layer(outputs, attn_mask)\n",
        "            attn_probs.append(attn_prob)\n",
        "        # (bs, n_enc_seq, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
        "        return outputs, attn_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCQJiVnBO7fu",
        "colab_type": "text"
      },
      "source": [
        "# 3. BERT\n",
        "BERT 모델 클래스 입니다.\n",
        "\n",
        "- 1. BERT는 Transformer Encoder를 실행합니다. (줄: 14)\n",
        "- 2. 1번의 결과(outputs)의 첫 번째([CLS]) Token을 outputs_cls로 저장 합니다. (줄: 16)\n",
        "- 3. outputs_cls에 Linear 및 tanh를 실행합니다. (줄: 17, 18)\n",
        "- 4. Pretrain된 모델을 저장하기위한 save 함수 입니다. (줄: 22 ~ 27)\n",
        "- 5. Pretrain된 모델을 읽기위한 load 함수 입니다. (줄: 29 ~ 32)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCqVBoFWPCuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" bert \"\"\"\n",
        "class BERT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.encoder = Encoder(self.config)\n",
        "\n",
        "        self.linear = nn.Linear(config.d_hidn, config.d_hidn)\n",
        "        self.activation = torch.tanh\n",
        "    \n",
        "    def forward(self, inputs, segments):\n",
        "        # (bs, n_seq, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
        "        outputs, self_attn_probs = self.encoder(inputs, segments)\n",
        "        # (bs, d_hidn)\n",
        "        outputs_cls = outputs[:, 0].contiguous()\n",
        "        outputs_cls = self.linear(outputs_cls)\n",
        "        outputs_cls = self.activation(outputs_cls)\n",
        "        # (bs, n_enc_seq, n_enc_vocab), (bs, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
        "        return outputs, outputs_cls, self_attn_probs\n",
        "    \n",
        "    def save(self, epoch, loss, path):\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"loss\": loss,\n",
        "            \"state_dict\": self.state_dict()\n",
        "        }, path)\n",
        "    \n",
        "    def load(self, path):\n",
        "        save = torch.load(path)\n",
        "        self.load_state_dict(save[\"state_dict\"])\n",
        "        return save[\"epoch\"], save[\"loss\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aDdNHpQPIbw",
        "colab_type": "text"
      },
      "source": [
        "# 4. Pretrain Model\n",
        "\n",
        "\n",
        "BERT를 Pretrain을 위한 목적함수는 다음 두 가지 입니다.\n",
        "\n",
        "- MLM (Masked Language Model)\n",
        "    \n",
        "    위 그림과 같이 [MASK]된 부분의 단어를 예측하는 것을 MLM이라 합니다.\n",
        "    전체 단어의 15%를 선택한 후 그중 80%는 [MASK], 10%는 현재 단어 유지, 10%는 임의의 단어로 대체 합니다.\n",
        "\n",
        "- NSP (Next Sentence Prediction)\n",
        "    \n",
        "    위 그림과 같이 첫 번째([CLS]) Token으로 문장 A와 문장 B의 관계를 예측하는 것을 NSP라 합니다.\n",
        "    \n",
        "    A 다음문장이 B가 맞을 경우는 True, A 다음문장이 B가 아닐 경우 False로 예측하도록 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5LHPtCOPXUG",
        "colab_type": "text"
      },
      "source": [
        "BERT를 Pretrain 하기위한 클래스 입니다.\n",
        "\n",
        "- 1. BERT의 결과를 입력으로 NSP를 예측하기위한 projection_cls를 선언합니다. (줄: 9)\n",
        "- 2. BERT의 결과를 입력으로 MLM을 예측하기위한 projection_lm을 선언합니다. (줄: 11)\n",
        "- 3. projection_lm은 Encoder의 Embedding과 weight를 share 합니다. (줄: 12)\n",
        "- 4. inputs, segments를 입력으로 BERT를 실행합니다. (줄: 16)\n",
        "- 5. outputs_cls를 입력으로 projection_cls를 실행하여 NSP를 예측하도록 합니다. (줄: 18)\n",
        "- 6. outputs를 입력으로 projection_lm을 실행하여 MLM을 예측하도록 합니다. (줄: 20)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haGf4uSrPS65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" BERT pretrain \"\"\"\n",
        "class BERTPretrain(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.bert = BERT(self.config)\n",
        "        # classfier\n",
        "        self.projection_cls = nn.Linear(self.config.d_hidn, 2, bias=False)\n",
        "        # lm\n",
        "        self.projection_lm = nn.Linear(self.config.d_hidn, self.config.n_enc_vocab, bias=False)\n",
        "        self.projection_lm.weight = self.bert.encoder.enc_emb.weight\n",
        "    \n",
        "    def forward(self, inputs, segments):\n",
        "        # (bs, n_enc_seq, d_hidn), (bs, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
        "        outputs, outputs_cls, attn_probs = self.bert(inputs, segments)\n",
        "        # (bs, 2)\n",
        "        logits_cls = self.projection_cls(outputs_cls)\n",
        "        # (bs, n_enc_seq, n_enc_vocab)\n",
        "        logits_lm = self.projection_lm(outputs)\n",
        "        # (bs, n_enc_vocab), (bs, n_enc_seq, n_enc_vocab), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
        "        return logits_cls, logits_lm, attn_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t90djukZPhRs",
        "colab_type": "text"
      },
      "source": [
        "# 5. Pretrain Data 생성\n",
        "**[MASK] 생성 함수**\n",
        "\n",
        "마스크를 생성하는 함수 입니다.\n",
        "\n",
        "- 1. token을 단어별로 index 배열 행태로 저장 합니다. (줄: 4 ~ 10)\n",
        "u”\\u2581”은 단어의 시작을 의미하는 값으로 시작하지 않으면 이전 Token과 연결된 subword 입니다.\n",
        "- 2. Random 선택을 위해 단어의 index를 섞습니다. (줄: 11)\n",
        "- 3. index를 for loop를 돌며 아래내용(4 ~ 9)을 실행 합니다. (줄: 10)\n",
        "- 4. mask_lms의 개수가 mask_cnt를 넘지 않도록 합니다. (줄: 15 ~ 18)\n",
        "mask_cnt는 전체 token개수의 15%에 해당하는 개수 입니다.\n",
        "- 5. index에 대해 80% 확률로 [MASK]를 취합니다. (줄: 21, 22)\n",
        "- 6. index에 대해 10% 확률로 현재 값을 유지 합니다. (줄: 24, 25)\n",
        "- 7. index에 대해 10% 확률로 vocab_list에서 임의의 값을 선택합니다. (줄: 26, 27)\n",
        "- 8. mask된 index의 값과 정답 label을 mask_lms에 저장 합니다. (줄: 28)\n",
        "- 9. token index의 값을 mask 합니다. (줄: 29)\n",
        "- 10. Random하게 mask된 값을 index순으로 정렬 합니다. (줄: 30)\n",
        "- 11. 10번에서 정렬된 값을 이용해 mask_index, mask_label을 만듭니다. (줄 31, 32)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLcOsW3vPuIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" 마스크 생성 \"\"\"\n",
        "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
        "    cand_idx = []\n",
        "    for (i, token) in enumerate(tokens):\n",
        "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
        "            continue\n",
        "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n",
        "            cand_idx[-1].append(i)\n",
        "        else:\n",
        "            cand_idx.append([i])\n",
        "    shuffle(cand_idx)\n",
        "\n",
        "    mask_lms = []\n",
        "    for index_set in cand_idx:\n",
        "        if len(mask_lms) >= mask_cnt:\n",
        "            break\n",
        "        if len(mask_lms) + len(index_set) > mask_cnt:\n",
        "            continue\n",
        "        for index in index_set:\n",
        "            masked_token = None\n",
        "            if random() < 0.8: # 80% replace with [MASK]\n",
        "                masked_token = \"[MASK]\"\n",
        "            else:\n",
        "                if random() < 0.5: # 10% keep original\n",
        "                    masked_token = tokens[index]\n",
        "                else: # 10% random word\n",
        "                    masked_token = choice(vocab_list)\n",
        "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
        "            tokens[index] = masked_token\n",
        "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
        "    mask_idx = [p[\"index\"] for p in mask_lms]\n",
        "    mask_label = [p[\"label\"] for p in mask_lms]\n",
        "\n",
        "    return tokens, mask_idx, mask_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTmXf_WePw-F",
        "colab_type": "text"
      },
      "source": [
        "## 최대 길이 초과하는 Token 자르기\n",
        "\n",
        "token A, token B의 길이의 합이 특정 길이보다 클 경우 이를 줄이는 함수입니다.\n",
        "\n",
        "- 1. token A의 길이가 길 경우 앞에서부터 토큰을 제거 합니다. (줄: 8, 9)\n",
        "- 2. token B의 길이가 길 경우 뒤에서부터 토큰을 제거 합니다. (줄: 10, 11)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVefhklOPzrg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" 최대 길이 초과하는 토큰 자르기 \"\"\"\n",
        "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_seq:\n",
        "            break\n",
        "\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            del tokens_a[0]\n",
        "        else:\n",
        "            tokens_b.pop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBaQ4BE6P3ND",
        "colab_type": "text"
      },
      "source": [
        "## 단락별 pretrain 데이터 생성 함수\n",
        "단락을 여러 개의 Pretrain 데이터로 만드는 함수 입니다.\n",
        "\n",
        "- 1. 특수 Token은 시작 ‘[CLS]’, 구분자 ‘[SEP]’ 2개 입니다.\n",
        "tgt_seq는 n_seq에서 3개를 뺀 값입니다. (줄: 4, 5)\n",
        "- 2. 단락을 줄 단위로 for loop를 돌며 아래내용(3 ~ 12)을 실행 합니다. (줄: 10)\n",
        "- 3. current_chunk에 line을 추가, current_length에 라인의 token 수를 더합니다. (줄: 11, 12)\n",
        "- 4. 마지막 줄 이거나 current_length가 tgt_seq를 넘을 경우 학습데이터를 만듭니다. (줄: 13)\n",
        "- 5. current_chunk에서 Random하게 길이를 선택해서 tokens_a를 만듭니다. (줄: 15 ~ 20)\n",
        "- 6. 50%의 확률로 다른 단락에서 tokens_b를 만듭니다. (줄: 23 ~ 33)\n",
        "is_next의 값은 False(0) 입니다. (줄: 24)\n",
        "- 7. 50%의 확률로 current_chunk에서 tokens_a 이후부터 tokens_b를 만듭니다. (줄: 34 ~ 37)\n",
        "is_next의 값은 True(1) 입니다. (줄: 35)\n",
        "- 8. 위에서 정의한 trim_tokens 함수를 실행하여 token 크기를 줄입니다. (줄: 39)\n",
        "- 9. ‘[CLS]’ + tokens_a + ‘[SEP]’ + tokens_b + ‘[SEP]’ 형태로 데이터를 생성 합니다. (줄: 43)\n",
        "- 10. segment를 생성 합니다. (줄: 44)\n",
        "9번에서 ‘[CLS]’ + tokens_a + ‘[SEP]’는 0, tokens_b + ‘[SEP]’는 1 입니다.\n",
        "- 11. 위에서 정의한 create_pretrain_mask 함수를 실행하여 Mask 합니다. (줄: 46)\n",
        "Mask Token 개수는 전체 Token 수에 0.15(15%)를 곱해서 구합니다.\n",
        "- 12. 위 결과를 가지고 데이터를 생성합니다. (줄: 48 ~ 55)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTCdynzYQFu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" doc별 pretrain 데이터 생성 \"\"\"\n",
        "def create_pretrain_instances(docs, doc_idx, doc, n_seq, mask_prob, vocab_list):\n",
        "    # for [CLS], [SEP], [SEP]\n",
        "    max_seq = n_seq - 3\n",
        "    tgt_seq = max_seq\n",
        "    \n",
        "    instances = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "    for i in range(len(doc)):\n",
        "        current_chunk.append(doc[i]) # line\n",
        "        current_length += len(doc[i])\n",
        "        if i == len(doc) - 1 or current_length >= tgt_seq:\n",
        "            if 0 < len(current_chunk):\n",
        "                a_end = 1\n",
        "                if 1 < len(current_chunk):\n",
        "                    a_end = randrange(1, len(current_chunk))\n",
        "                tokens_a = []\n",
        "                for j in range(a_end):\n",
        "                    tokens_a.extend(current_chunk[j])\n",
        "                \n",
        "                tokens_b = []\n",
        "                if len(current_chunk) == 1 or random() < 0.5:\n",
        "                    is_next = 0\n",
        "                    tokens_b_len = tgt_seq - len(tokens_a)\n",
        "                    random_doc_idx = doc_idx\n",
        "                    while doc_idx == random_doc_idx:\n",
        "                        random_doc_idx = randrange(0, len(docs))\n",
        "                    random_doc = docs[random_doc_idx]\n",
        "\n",
        "                    random_start = randrange(0, len(random_doc))\n",
        "                    for j in range(random_start, len(random_doc)):\n",
        "                        tokens_b.extend(random_doc[j])\n",
        "                else:\n",
        "                    is_next = 1\n",
        "                    for j in range(a_end, len(current_chunk)):\n",
        "                        tokens_b.extend(current_chunk[j])\n",
        "\n",
        "                trim_tokens(tokens_a, tokens_b, max_seq)\n",
        "                assert 0 < len(tokens_a)\n",
        "                assert 0 < len(tokens_b)\n",
        "\n",
        "                tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
        "                segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
        "\n",
        "                tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n",
        "\n",
        "                instance = {\n",
        "                    \"tokens\": tokens,\n",
        "                    \"segment\": segment,\n",
        "                    \"is_next\": is_next,\n",
        "                    \"mask_idx\": mask_idx,\n",
        "                    \"mask_label\": mask_label\n",
        "                }\n",
        "                instances.append(instance)\n",
        "\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "    return instances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_l9J7L4QKN4",
        "colab_type": "text"
      },
      "source": [
        "## pretrain 데이터 생성 함수\n",
        "\n",
        "말뭉치를 읽어 Pretrain 데이터를 만드는 함수 입니다.\n",
        "\n",
        "- 1. 단어목록 vocab_list를 생성 합니다. 생성 시 unknown은 제거합니다. (줄: 3 ~ 6)\n",
        "vocab_list는 위에서 정의한 create_pretrain_mask 함수의 입력으로 사용하기 위함 입니다.\n",
        "- 2. 말뭉치 파일 라인수를 확인 합니다. (줄: 8 ~ 11)\n",
        "- 3. 말뭉치를 줄 단위로 for loop를 돌며 아래내용(3 ~ 4)을 실행 합니다. (줄: 14)\n",
        "줄의 문자를 vocab을 이용해 tokenize한 후 doc에 추가 합니다. (줄: 24 ~ 26)\n",
        "- 4. 빈 줄이 나타날 경우 단락의 끝이므로 doc를 docs에 추가하고 doc를 새로 만듭니다. (줄: 19 ~ 22)\n",
        "- 5. count 횟수만큼 for loop를 돌며 (7 ~ 9) Pretrain 데이터를 만듭니다. (줄: 31)\n",
        "BERT는 Mask를 15%만 하므로 한 번에 전체 단어를 학습할 수 없습니다. 한 말뭉치에 대해 통상 Pretrain 데이터 10개(150%) 정도 만들어서 학습하도록 합니다.\n",
        "- 6. docs(단락배열)을 doc(단락) 단위로 for loop를 돌며 아래내용(6 ~ 7)을 실행 합니다. (줄: 28)\n",
        "- 7. doc를 입력으로 위에서 정의한 create_pretrain_instances 함수를 호출합니다. (줄: 37)\n",
        "- 8. 8변의 결과를 파일에 저장합니다. (줄: 39 ~ 41)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQjxCYBZQUnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" pretrain 데이터 생성 \"\"\"\n",
        "def make_pretrain_data(vocab, in_file, out_file, count, n_seq, mask_prob):\n",
        "    vocab_list = []\n",
        "    for id in range(vocab.get_piece_size()):\n",
        "        if not vocab.is_unknown(id):\n",
        "            vocab_list.append(vocab.id_to_piece(id))\n",
        "\n",
        "    line_cnt = 0\n",
        "    with open(in_file, \"r\") as in_f:\n",
        "        for line in in_f:\n",
        "            line_cnt += 1\n",
        "    \n",
        "    docs = []\n",
        "    with open(in_file, \"r\") as f:\n",
        "        doc = []\n",
        "        with tqdm(total=line_cnt, desc=f\"Loading\") as pbar:\n",
        "            for i, line in enumerate(f):\n",
        "                line = line.strip()\n",
        "                if line == \"\":\n",
        "                    if 0 < len(doc):\n",
        "                        docs.append(doc)\n",
        "                        doc = []\n",
        "                else:\n",
        "                    pieces = vocab.encode_as_pieces(line)\n",
        "                    if 0 < len(pieces):\n",
        "                        doc.append(pieces)\n",
        "                pbar.update(1)\n",
        "        if doc:\n",
        "            docs.append(doc)\n",
        "\n",
        "    for index in range(count):\n",
        "        output = out_file.format(index)\n",
        "        if os.path.isfile(output): continue\n",
        "\n",
        "        with open(output, \"w\") as out_f:\n",
        "            with tqdm(total=len(docs), desc=f\"Making\") as pbar:\n",
        "                for i, doc in enumerate(docs):\n",
        "                    instances = create_pretrain_instances(docs, i, doc, n_seq, mask_prob, vocab_list)\n",
        "                    for instance in instances:\n",
        "                        out_f.write(json.dumps(instance))\n",
        "                        out_f.write(\"\\n\")\n",
        "                    pbar.update(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpczLvzHQXUK",
        "colab_type": "text"
      },
      "source": [
        "## pretrain 데이터 생성 실행\n",
        "\n",
        "pretrain 데이터를 만드는 코드 입니다.\n",
        "\n",
        "- 말뭉치 개수(count)는 10로 합니다.\n",
        "- sequence 길이(n_seq)는 256으로 합니다.\n",
        "- Mask 확률(mask_prob)는 15%로 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw5fS7fcQaK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "in_file = \"<path of data>/kowiki.txt\"\n",
        "out_file = \"<path of data>/kowiki_bert_{}.json\"\n",
        "count = 10\n",
        "n_seq = 256\n",
        "mask_prob = 0.15\n",
        "\n",
        "make_pretrain_data(vocab, in_file, out_file, count, n_seq, mask_prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IbFGWLzQghq",
        "colab_type": "text"
      },
      "source": [
        "# 6. DataSet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9IihAWQQ0Ru",
        "colab_type": "text"
      },
      "source": [
        "## DataSet\n",
        "\n",
        "Pretrain DataSet 입니다.\n",
        "\n",
        "- 1. 입력 파일에서 아래 내용을 읽습니다. (줄: 18 ~ 23)\n",
        "    \n",
        "    is_next: tokens_a와 tokens_b가 인접한 문장인지 여부\n",
        "    \n",
        "    tokens: 문장 tokens\n",
        "    \n",
        "    segment: tokens_a(0)와 tokens_b(1)을 구분하기 위한 값\n",
        "    \n",
        "    mask_idx: tokens내의 mask index\n",
        "    \n",
        "    mask_label: tokens내의 mask된 부분의 정답\n",
        "    \n",
        "- 2. 값이 모두 -1인 label_lm 변수를 만듭니다. (줄: 24)\n",
        "- 3. label_lm의 mask_idx 위치에 mask_label값을 저장 합니다. (줄: 25)\n",
        "이렇게 하면 mask_idx 위치는 mask_label이 나머지는 -1이 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzzFQoaBQqlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" pretrain 데이터셋 \"\"\"\n",
        "class PretrainDataSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, vocab, infile):\n",
        "        self.vocab = vocab\n",
        "        self.labels_cls = []\n",
        "        self.labels_lm = []\n",
        "        self.sentences = []\n",
        "        self.segments = []\n",
        "\n",
        "        line_cnt = 0\n",
        "        with open(infile, \"r\") as f:\n",
        "            for line in f:\n",
        "                line_cnt += 1\n",
        "\n",
        "        with open(infile, \"r\") as f:\n",
        "            for i, line in enumerate(tqdm(f, total=line_cnt, desc=f\"Loading {infile}\", unit=\" lines\")):\n",
        "                instance = json.loads(line)\n",
        "                self.labels_cls.append(instance[\"is_next\"])\n",
        "                sentences = [vocab.piece_to_id(p) for p in instance[\"tokens\"]]\n",
        "                self.sentences.append(sentences)\n",
        "                self.segments.append(instance[\"segment\"])\n",
        "                mask_idx = np.array(instance[\"mask_idx\"], dtype=np.int)\n",
        "                mask_label = np.array([vocab.piece_to_id(p) for p in instance[\"mask_label\"]], dtype=np.int)\n",
        "                label_lm = np.full(len(sentences), dtype=np.int, fill_value=-1)\n",
        "                label_lm[mask_idx] = mask_label\n",
        "                self.labels_lm.append(label_lm)\n",
        "    \n",
        "    def __len__(self):\n",
        "        assert len(self.labels_cls) == len(self.labels_lm)\n",
        "        assert len(self.labels_cls) == len(self.sentences)\n",
        "        assert len(self.labels_cls) == len(self.segments)\n",
        "        return len(self.labels_cls)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        return (torch.tensor(self.labels_cls[item]),\n",
        "                torch.tensor(self.labels_lm[item]),\n",
        "                torch.tensor(self.sentences[item]),\n",
        "                torch.tensor(self.segments[item]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPnmQrLBQt3N",
        "colab_type": "text"
      },
      "source": [
        "## collate_fn\n",
        "\n",
        "배치단위로 데이터 처리를 위한 collate_fn 입니다.\n",
        "\n",
        "- 1. labels_lm의 길이가 같아지도록 짧은 문장에 padding(-1)을 추가 합니다. (줄: 5)\n",
        "- 2. inputs의 길이가 같아지도록 짧은 문장에 padding(0)을 추가 합니다. (줄: 6)\n",
        "padding은 Sentencepiece를 활용해 Vocab 만들기에서 ‘–pad_id=0’옵션으로 지정한 값 입니다.\n",
        "- 3. segments의 길이가 같아지도록 짧은 문장에 padding(0)을 추가 합니다. (줄: 7)\n",
        "- 4. labels_cls는 길이가 1 고정이므로 stack 함수를 이용해 tensor로 만듭니다. (줄: 10)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv6bLdjkQ5HF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" pretrain data collate_fn \"\"\"\n",
        "def pretrin_collate_fn(inputs):\n",
        "    labels_cls, labels_lm, inputs, segments = list(zip(*inputs))\n",
        "\n",
        "    labels_lm = torch.nn.utils.rnn.pad_sequence(labels_lm, batch_first=True, padding_value=-1)\n",
        "    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
        "    segments = torch.nn.utils.rnn.pad_sequence(segments, batch_first=True, padding_value=0)\n",
        "\n",
        "    batch = [\n",
        "        torch.stack(labels_cls, dim=0),\n",
        "        labels_lm,\n",
        "        inputs,\n",
        "        segments\n",
        "    ]\n",
        "    return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aorJ0GeGQ7js",
        "colab_type": "text"
      },
      "source": [
        "## DataLoader\n",
        "위에서 정의한 DataSet과 collate_fn을 이용해 학습용(train_loader) DataLoader를 만듭니다.\n",
        "\n",
        "위에서 생성한 pretrain 데이터 중 첫 번째 값을 읽습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR05u5P4RB5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" pretrain 데이터 로더 \"\"\"\n",
        "batch_size = 128\n",
        "dataset = PretrainDataSet(vocab, f\"{data_dir}/kowiki_bert_0.json\")\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=pretrin_collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_gtE19ARAER",
        "colab_type": "text"
      },
      "source": [
        "# 7. Pretrain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM6O1XG6RG5P",
        "colab_type": "text"
      },
      "source": [
        "## Train\n",
        "\n",
        "BERT 모델을 Pretrain 하기 위한 함수 입니다....\n",
        "\n",
        "- 1. inputs, segements를 입력으로 BERTPretrain을 실행합니다. (줄: 11)\n",
        "- 2. 1번의 결과 중 첫 번째 값이 NSP(logits_cls), 두 번째 값이 MLM(logits_lm) 입니다. (줄: 12)\n",
        "- 3. logits_cls 값과 labels_cls 값을 이용해 NSP Loss(loss_cls)를 계산합니다. (줄: 14)\n",
        "- 4. logits_lm 값과 labels_lm 값을 이용해 MLM Loss(loss_lm)를 계산합니다. (줄: 15)\n",
        "- 5. loss_cls와 loss_lm을 더해 loss를 생성합니다. (줄: 16)\n",
        "- 6. loss, optimizer를 이용해 학습합니다. (줄: 21, 22)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc5owooXQ9Ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" 모델 epoch 학습 \"\"\"\n",
        "def train_epoch(config, epoch, model, criterion_lm, criterion_cls, optimizer, train_loader):\n",
        "    losses = []\n",
        "    model.train()\n",
        "\n",
        "    with tqdm(total=len(train_loader), desc=f\"Train({epoch})\") as pbar:\n",
        "        for i, value in enumerate(train_loader):\n",
        "            labels_cls, labels_lm, inputs, segments = map(lambda v: v.to(config.device), value)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs, segments)\n",
        "            logits_cls, logits_lm = outputs[0], outputs[1]\n",
        "\n",
        "            loss_cls = criterion_cls(logits_cls, labels_cls)\n",
        "            loss_lm = criterion_lm(logits_lm.view(-1, logits_lm.size(2)), labels_lm.view(-1))\n",
        "            loss = loss_cls + loss_lm\n",
        "\n",
        "            loss_val = loss_lm.item()\n",
        "            losses.append(loss_val)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix_str(f\"Loss: {loss_val:.3f} ({np.mean(losses):.3f})\")\n",
        "    return np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUO63zwSRPuh",
        "colab_type": "text"
      },
      "source": [
        "학습을 위한 추가적인 내용을 선언 합니다.\n",
        "\n",
        "- 1. GPU 사용 여부를 확인합니다. (줄: 1)\n",
        "- 2. learning_rate 및 학습 epoch를 선언 합니다. (줄: 4, 5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46Lx16YDRScS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(config)\n",
        "\n",
        "learning_rate = 5e-5\n",
        "n_epoch = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUwxhtBvRW-_",
        "colab_type": "text"
      },
      "source": [
        "위에서 선언된 내용을 이용해 학습을 실행하는 절차 입니다.\n",
        "\n",
        "- 1. BERTPretrain을 생성합니다. (줄: 1)\n",
        "- 2. 기존에 학습된 pretrain 값이 있다면 이를 로드 합니다. (줄: 5 ~ 8)\n",
        "- 3. BERTPretrain이 GPU 또는 CPU를 지원하도록 합니다. (줄: 10)\n",
        "- 4. MLM loss(criterion_lm) 및 NLP loss(criterion_cls) 함수를 선언 합니다. (줄: 12, 13)\n",
        "- 5. optimizer를 선언 합니다. (줄: 14)\n",
        "- 6. 각 epoch 마다 새로 train_loader를 생성 합니다. (줄: 20 ~ 23)\n",
        "step이 0인 경우는 위에서 생성했기 때문에 생성하지 않습니다.\n",
        "- 7. 각 epoch 마다 학습을 합니다. (줄: 25)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olQc66hSRdAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BERTPretrain(config)\n",
        "\n",
        "save_pretrain = f\"{data_dir}/save_bert_pretrain.pth\"\n",
        "best_epoch, best_loss = 0, 0\n",
        "if os.path.isfile(save_pretrain):\n",
        "    best_epoch, best_loss = model.bert.load(save_pretrain)\n",
        "    print(f\"load pretrain from: {save_pretrain}, epoch={best_epoch}, loss={best_loss}\")\n",
        "    best_epoch += 1\n",
        "\n",
        "model.to(config.device)\n",
        "\n",
        "criterion_lm = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='mean')\n",
        "criterion_cls = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "losses = []\n",
        "offset = best_epoch\n",
        "for step in range(n_epoch):\n",
        "    epoch = step + offset\n",
        "    if 0 < step:\n",
        "        del train_loader\n",
        "        dataset = PretrainDataSet(vocab, f\"{data_dir}/kowiki_bert_{epoch % count}.json\")\n",
        "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=pretrin_collate_fn)\n",
        "\n",
        "    loss = train_epoch(config, epoch, model, criterion_lm, criterion_cls, optimizer, train_loader)\n",
        "    losses.append(loss)\n",
        "    model.bert.save(epoch, loss, save_pretrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opdhveeURf3C",
        "colab_type": "text"
      },
      "source": [
        "# 8. Result\n",
        "\n",
        "학습결과는 아래와 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFn0_WKXRg55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##data\n",
        "data = {\n",
        "    \"loss\": losses\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "display(df)\n",
        "\n",
        "# graph\n",
        "plt.figure(figsize=[8, 4])\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Depth')\n",
        "plt.xlim((0, n_epoch - 1))\n",
        "plt.ylabel('Position')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znA571gNRu_0",
        "colab_type": "text"
      },
      "source": [
        "# 9. Model\n",
        "\n",
        "BERT 클래스를 이용하여 영수증 브랜드 분류 모델 클래스를 아래와 같이 정의 합니다.\n",
        "\n",
        "- 1. inputs, segments를 입력으로 BERT 모델을 실행 합니다. (줄: 13)\n",
        "- 2. 1변 결과의 outputs_cls 값을 Classification을 위한 값으로 사용합니다. (줄: 15)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N4b7EWIR2jW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" naver movie classfication \"\"\"\n",
        "class MovieClassification(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.bert = BERT(self.config)\n",
        "        # classfier\n",
        "        self.projection_cls = nn.Linear(self.config.d_hidn, self.config.n_output, bias=False)\n",
        "    \n",
        "    def forward(self, inputs, segments):\n",
        "        # (bs, n_enc_seq, d_hidn), (bs, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
        "        outputs, outputs_cls, attn_probs = self.bert(inputs, segments)\n",
        "        # (bs, n_output)\n",
        "        logits_cls = self.projection_cls(outputs_cls)\n",
        "        # (bs, n_output), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
        "        return logits_cls, attn_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4rzF5uVR6CV",
        "colab_type": "text"
      },
      "source": [
        "# 10. DataSet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnFfwfvPR-Z5",
        "colab_type": "text"
      },
      "source": [
        "## DataSet\n",
        "\n",
        "영수증 브랜드 별 OCR 결과 데이터 셋 입니다.\n",
        "\n",
        "- 1. 입력 파일로 부터 ‘label’을 읽어 들입니다. (줄: 17)\n",
        "- 2. 입력 파일로 부터 ‘doc’ token을 읽어 숫자(token id)로 변경 합니다. (줄: 18, 19)\n",
        "위 그림과 같이 시작은 ‘[CLS]’ 끝은 ‘[SEP]’가 되도록 합니다.\n",
        "- 3. segment(0)를 생성합니다. (줄: 20)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejB4WfrAR921",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" 영화 분류 데이터셋 \"\"\"\n",
        "class MovieDataSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, vocab, infile):\n",
        "        self.vocab = vocab\n",
        "        self.labels = []\n",
        "        self.sentences = []\n",
        "        self.segments = []\n",
        "\n",
        "        line_cnt = 0\n",
        "        with open(infile, \"r\") as f:\n",
        "            for line in f:\n",
        "                line_cnt += 1\n",
        "\n",
        "        with open(infile, \"r\") as f:\n",
        "            for i, line in enumerate(tqdm(f, total=line_cnt, desc=\"Loading Dataset\", unit=\" lines\")):\n",
        "                data = json.loads(line)\n",
        "                self.labels.append(data[\"label\"])\n",
        "                sentence = [vocab.piece_to_id(\"[CLS]\")] + [vocab.piece_to_id(p) for p in data[\"doc\"]] + [vocab.piece_to_id(\"[SEP]\")]\n",
        "                self.sentences.append(sentence)\n",
        "                self.segments.append([0] * len(sentence))\n",
        "    \n",
        "    def __len__(self):\n",
        "        assert len(self.labels) == len(self.sentences)\n",
        "        assert len(self.labels) == len(self.segments)\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        return (torch.tensor(self.labels[item]),\n",
        "                torch.tensor(self.sentences[item]),\n",
        "                torch.tensor(self.segments[item]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5ag8HH8SLsX",
        "colab_type": "text"
      },
      "source": [
        "## collate_fn\n",
        "\n",
        "배치단위로 데이터 처리를 위한 collate_fn 입니다.\n",
        "\n",
        "- 1. inputs의 길이가 같아지도록 짧은 문장에 padding(0)을 추가 합니다. (줄: 5)\n",
        "padding은 Sentencepiece를 활용해 Vocab 만들기에서 ‘–pad_id=0’옵션으로 지정한 값 입니다.\n",
        "- 2. segments의 길이가 같아지도록 짧은 문장에 padding(0)을 추가 합니다. (줄: 6)\n",
        "- 3. Label은 길이가 1 고정이므로 stack 함수를 이용해 tensor로 만듭니다. (줄: 9)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwaKyhlrSPQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" movie data collate_fn \"\"\"\n",
        "def movie_collate_fn(inputs):\n",
        "    labels, inputs, segments = list(zip(*inputs))\n",
        "\n",
        "    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
        "    segments = torch.nn.utils.rnn.pad_sequence(segments, batch_first=True, padding_value=0)\n",
        "\n",
        "    batch = [\n",
        "        torch.stack(labels, dim=0),\n",
        "        inputs,\n",
        "        segments,\n",
        "    ]\n",
        "    return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5t8mmf2SRgC",
        "colab_type": "text"
      },
      "source": [
        "## DataLoader\n",
        "\n",
        "위에서 정의한 DataSet과 collate_fn을 이용해 학습용(train_loader), 평가용(test_loader) DataLoader를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFTZMyidSTDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" 데이터 로더 \"\"\"\n",
        "batch_size = 128\n",
        "train_dataset = MovieDataSet(vocab, f\"{data_dir}/ratings_train.json\")\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=movie_collate_fn)\n",
        "test_dataset = MovieDataSet(vocab, f\"{data_dir}/ratings_test.json\")\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=movie_collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOkNJ3ihSXH6",
        "colab_type": "text"
      },
      "source": [
        "# 11. Evaluate\n",
        "\n",
        "학습된 MovieClassification 모델의 성능을 평가하기 위한 함수 입니다. 평가는 정확도(accuracy)를 사용 했습니다.\n",
        "\n",
        "- 1. inputs, segments를 입력으로 MovieClassification을 실행합니다. (줄: 12)\n",
        "- 2. 1번의 결과 중 첫번째 값이 예측 logits 입니다. (줄: 13)\n",
        "- 3. logits의 최대값의 index를 구합니다. (줄: 14)\n",
        "- 4. 3번에게 구한 값과 labels의 값이 같은지 비교 합니다. (줄: 16)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPdXRcAtSdG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" 모델 epoch 평가 \"\"\"\n",
        "def eval_epoch(config, model, data_loader):\n",
        "    matchs = []\n",
        "    model.eval()\n",
        "\n",
        "    n_word_total = 0\n",
        "    n_correct_total = 0\n",
        "    with tqdm(total=len(data_loader), desc=f\"Valid\") as pbar:\n",
        "        for i, value in enumerate(data_loader):\n",
        "            labels, inputs, segments = map(lambda v: v.to(config.device), value)\n",
        "\n",
        "            outputs = model(inputs, segments)\n",
        "            logits_cls = outputs[0]\n",
        "            _, indices = logits_cls.max(1)\n",
        "\n",
        "            match = torch.eq(indices, labels).detach()\n",
        "            matchs.extend(match.cpu())\n",
        "            accuracy = np.sum(matchs) / len(matchs) if 0 < len(matchs) else 0\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix_str(f\"Acc: {accuracy:.3f}\")\n",
        "    return np.sum(matchs) / len(matchs) if 0 < len(matchs) else 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMAzqR_4SpU3",
        "colab_type": "text"
      },
      "source": [
        "# 12. Train\n",
        "\n",
        "MovieClassification 모델을 학습하기 위한 함수 입니다.\n",
        "\n",
        "- 1. inputs, segments를 입력으로 MovieClassification을 실행합니다. (줄: 11)\n",
        "- 2. 1번의 결과 중 첫 번째 값이 예측 logits 입니다. (줄: 12)\n",
        "- 3. logits 값과 labels의 값을 이용해 Loss를 계산합니다. (줄: 14)\n",
        "- 4. loss, optimizer를 이용해 학습합니다. (줄: 20, 21)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYbcSPwDSxkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" 모델 epoch 학습 \"\"\"\n",
        "def train_epoch(config, epoch, model, criterion_cls, optimizer, train_loader):\n",
        "    losses = []\n",
        "    model.train()\n",
        "\n",
        "    with tqdm(total=len(train_loader), desc=f\"Train({epoch})\") as pbar:\n",
        "        for i, value in enumerate(train_loader):\n",
        "            labels, inputs, segments = map(lambda v: v.to(config.device), value)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs, segments)\n",
        "            logits_cls = outputs[0]\n",
        "\n",
        "            loss_cls = criterion_cls(logits_cls, labels)\n",
        "            loss = loss_cls\n",
        "\n",
        "            loss_val = loss_cls.item()\n",
        "            losses.append(loss_val)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix_str(f\"Loss: {loss_val:.3f} ({np.mean(losses):.3f})\")\n",
        "    return np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUYg9yLISz-S",
        "colab_type": "text"
      },
      "source": [
        "학습을 위한 추가적인 내용을 선언 합니다.\n",
        "\n",
        "- 1. GPU 사용 여부를 확인합니다. (줄: 1)\n",
        "- 2. 출력 값 개수를 정의 합니다. (부정(0), 긍정(1) 2가지입니다.) (줄: 2)\n",
        "- 3. learning_rate 및 학습 epoch를 선언 합니다. (줄: 5, 6)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEXdSEqPS2R7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "config.n_output = 2\n",
        "print(config)\n",
        "\n",
        "learning_rate = 5e-5\n",
        "n_epoch = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1YG51uSS8Mp",
        "colab_type": "text"
      },
      "source": [
        "위에서 선언된 내용을 이용해 학습을 실행하는 함수입니다.\n",
        "\n",
        "- 1. MovieClassification이 GPU 또는 CPU를 지원하도록 합니다. (줄: 2)\n",
        "- 2. loss 함수를 선언 합니다. (줄: 4)\n",
        "- 3. optimizer를 선언 합니다. (줄: 5)\n",
        "- 4. 각 epoch 마다 학습을 합니다. (줄: 10)\n",
        "- 5. 각 epoch 마다 평가를 합니다. (줄: 11)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNX0TjunTAmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model):\n",
        "    model.to(config.device)\n",
        "\n",
        "    criterion_cls = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    best_epoch, best_loss, best_score = 0, 0, 0\n",
        "    losses, scores = [], []\n",
        "    for epoch in range(n_epoch):\n",
        "        loss = train_epoch(config, epoch, model, criterion_cls, optimizer, train_loader)\n",
        "        score = eval_epoch(config, model, test_loader)\n",
        "\n",
        "        losses.append(loss)\n",
        "        scores.append(score)\n",
        "\n",
        "        if best_score < score:\n",
        "            best_epoch, best_loss, best_score = epoch, loss, score\n",
        "    print(f\">>>> epoch={best_epoch}, loss={best_loss:.5f}, socre={best_score:.5f}\")\n",
        "    return losses, scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXAn3YTMTEMg",
        "colab_type": "text"
      },
      "source": [
        "## Train (No Pretrain)\n",
        "\n",
        "Pretrain을 사용하지 않고 학습을 진행 합니다.\n",
        "\n",
        "- 1. MovieClassification을 생성합니다. (줄: 1)\n",
        "- 2. 추가적인 처리 없이 생성된 MovieClassification으로 학습을 진행 합니다. (줄: 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwBHyeczTHPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MovieClassification(config)\n",
        "\n",
        "losses_00, scores_00 = train(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KAMeF7sTMTz",
        "colab_type": "text"
      },
      "source": [
        "## Train (20 epoch Pretrain)\n",
        "\n",
        "20 epoch Pretrain된 모델을 이용해 학습을 진행 합니다.\n",
        "\n",
        "- 1. MovieClassification을 생성합니다. (줄: 1)\n",
        "- 2. 앞에서 구현한 Pretrain 모델을 로드 합니다. (줄: 3, 4)\n",
        "- 3. MovieClassification으로 학습을 진행 합니다. (줄: 6)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl2QMmP9TTtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MovieClassification(config)\n",
        "\n",
        "save_pretrain = \"<path of data>/save_bert_pretrain.pth\"\n",
        "model.bert.load(save_pretrain)\n",
        "\n",
        "losses_20, scores_20 = train(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw7rLqpiTWV3",
        "colab_type": "text"
      },
      "source": [
        "# 13. Result\n",
        "학습결과 및 평가결과는 아래와 같습니다.\n",
        "\n",
        "Pretrain을 안한 경우는 정확도(score)가 81.8% 정도 나왔습니다.\n",
        "\n",
        "Pretrain을 20 epoch 한 경우는 정확도(score)가 81.9% 정도 나왔습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PPkA_46TY5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# table\n",
        "data = {\n",
        "    \"loss_00\": losses_00,\n",
        "    \"socre_00\": scores_00,\n",
        "    \"loss_20\": losses_20,\n",
        "    \"socre_20\": scores_20,\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "display(df)\n",
        "\n",
        "# graph\n",
        "plt.figure(figsize=[12, 4])\n",
        "plt.plot(scores_00, label=\"score_00\")\n",
        "plt.plot(scores_20, label=\"score_20\")\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Value')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}