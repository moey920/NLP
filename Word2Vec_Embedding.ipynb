{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec Embedding.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMYr1xRk8c6YZTKp1aNUb9m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moey920/NLP/blob/master/Word2Vec_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwhxkVjOEr9t",
        "colab_type": "text"
      },
      "source": [
        "# 텍스트 파일 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI3UAs4I7Wn_",
        "colab_type": "text"
      },
      "source": [
        "## 훈련 데이터셋 불러와 전처리하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfupF3aIGUd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZE_2B4zdsMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jtNtvuTWyx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_file_link = '/content/drive/My Drive/datt/CNN_텍스트분류_정답_수정.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNpiSh2OW8p_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 불러온 데이터를 보면 id, document, label로 구분이 되어있습니다.\n",
        "train_data = pd.read_csv(train_file_link, header = 0, delimiter = '\\t', quoting = 3)\n",
        "train_data.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJezxR68XSrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUZsFcVKXMof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import json\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5K58Cy230V2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(review, okt, remove_stopwords = False, stop_words = []):\n",
        "    # 함수의 인자는 다음과 같다.\n",
        "    # review : 전처리할 텍스트\n",
        "    # okt : okt 객체를 반복적으로 생성하지 않고 미리 생성후 인자로 받는다.\n",
        "    # remove_stopword : 불용어를 제거할지 선택 기본값은 False\n",
        "    # stop_word : 불용어 사전은 사용자가 직접 입력해야함 기본값은 비어있는 리스트\n",
        "    \n",
        "    # 1. 한글 및 공백을 제외한 문자 모두 제거. + 영어 소문자, 대문자, 숫자도 제외\n",
        "    # 일단 OCR 결과의 원형을 학습시키기 위해 정규표현식을 사용하지 않고 학습시켜보겠습니다.\n",
        "    review_text = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9\\\\s]\", \" \",  review)\n",
        "    #review_text = re.sub(\" \", \"\",  review)\n",
        "    \n",
        "    # 2. okt 객체를 활용해서 형태소 단위로 나눈다.\n",
        "    word_review = okt.morphs(review_text, stem=True)\n",
        "    \n",
        "    if remove_stopwords:\n",
        "        \n",
        "        # 불용어 제거(선택적)\n",
        "        word_review = [token for token in word_review if not token in stop_words]\n",
        "        \n",
        "   \n",
        "    return word_review"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljaUk1G732Zi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = ['은', '는', '이', '가', '하', '아', '것', '들','의', '있', '되', '수', '보', \n",
        "              '주', '등', '한', '(', ')', '/', '*', '=', 'E', '|', '-', '.', ',', 'II', 'لالالالا', \n",
        "              '|||||||||', 'iii', '|||', '. ', '.', '\"', ' )', '[', ']', '\"']\n",
        "okt = Okt()\n",
        "clean_train_review = []\n",
        "\n",
        "for review in tqdm(train_data['document']):\n",
        "    # 비어있는 데이터에서 멈추지 않도록 string인 경우만 진행\n",
        "    if type(review) == str:\n",
        "        clean_train_review.append(preprocessing(review, okt, remove_stopwords = True, stop_words=stop_words))\n",
        "    else:\n",
        "        clean_train_review.append([])  #string이 아니면 비어있는 값 추가"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iR850QhGX5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = clean_train_review\n",
        "y_train = train_data['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8IWXCMYGddc",
        "colab_type": "text"
      },
      "source": [
        "문장과 레이블 데이터를 만들었습니다. 레이블은 20개로 세분화되어있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWnWw11Rb5az",
        "colab_type": "text"
      },
      "source": [
        "## 테스트 파일 불러와 전처리하기\n",
        "\n",
        "해당 코드는 결과를 확인하기 위한 테스트 파일을 불러오는 코드입니다. \n",
        "훈련을 먼저 진행하고자하면 \n",
        "- 2) 임베딩 층 사용하기의 훈련파일 불러오기에서 전처리를 진행해주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGs-pmqIb8cY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_file_link = '/content/drive/My Drive/datt/testdataset2.txt' # 라벨값 없음"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-iVL_jQb4ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = pd.read_csv(test_file_link, header = 0, delimiter = '\\t', quoting = 3)\n",
        "test_data.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaD5ZjAxcFeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import json\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZQGz32bcWkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(review, okt, remove_stopwords = False, stop_words = []):\n",
        "    # 함수의 인자는 다음과 같다.\n",
        "    # review : 전처리할 텍스트\n",
        "    # okt : okt 객체를 반복적으로 생성하지 않고 미리 생성후 인자로 받는다.\n",
        "    # remove_stopword : 불용어를 제거할지 선택 기본값은 False\n",
        "    # stop_word : 불용어 사전은 사용자가 직접 입력해야함 기본값은 비어있는 리스트\n",
        "    \n",
        "    # 1. 한글 및 공백을 제외한 문자 모두 제거. + 영어 소문자, 대문자, 숫자도 제외\n",
        "    # 일단 OCR 결과의 원형을 학습시키기 위해 정규표현식을 사용하지 않고 학습시켜보겠습니다.\n",
        "    review_text = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9\\\\s]\", \" \",  review)\n",
        "    #review_text = re.sub(\" \", \"\",  review)\n",
        "    \n",
        "    # 2. okt 객체를 활용해서 형태소 단위로 나눈다.\n",
        "    word_review = okt.morphs(review_text, stem=True)\n",
        "    \n",
        "    if remove_stopwords:\n",
        "        \n",
        "        # 불용어 제거(선택적)\n",
        "        word_review = [token for token in word_review if not token in stop_words]\n",
        "        \n",
        "   \n",
        "    return word_review"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6Dqg2BhcbI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = ['은', '는', '이', '가', '하', '아', '것', '들','의', '있', '되', '수', '보', \n",
        "              '주', '등', '한', '(', ')', '/', '*', '=', 'E', '|', '-', '.', ',', 'II', 'لالالالا', \n",
        "              '|||||||||', 'iii', '|||', '. ', '.', '\"', ' )', '[', ']', '\"']\n",
        "okt = Okt()\n",
        "clean_test_review = []\n",
        "\n",
        "for review in tqdm(test_data['document']):\n",
        "    # 비어있는 데이터에서 멈추지 않도록 string인 경우만 진행\n",
        "    if type(review) == str:\n",
        "        clean_test_review.append(preprocessing(review, okt, remove_stopwords = True, stop_words=stop_words))\n",
        "    else:\n",
        "        clean_test_review.append([])  #string이 아니면 비어있는 값 추가"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWKphExCExV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iFqhsC-cywd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = clean_test_review\n",
        "y_train = test_data['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td6R1sRF7tfa",
        "colab_type": "text"
      },
      "source": [
        "## 토크나이징"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8TSylPBc_GD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = Tokenizer()\n",
        "t.fit_on_texts(sentences)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "\n",
        "print(vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tSmjmdrdF-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_encoded = t.texts_to_sequences(sentences)\n",
        "print(X_encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZWL2Jz0dZXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len=max(len(l) for l in X_encoded)\n",
        "print(max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC1C4n83dcP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train=pad_sequences(X_encoded, maxlen=max_len, padding='post')\n",
        "y_train=np.array(y_train)\n",
        "print(X_train)\n",
        "print(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmLx75lferjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 테스트 파일일 경우에만 실행(Nan 라벨을 모두 0으로 바꿈)\n",
        "y_train[:] = 0\n",
        "print(y_train)\n",
        "len(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0aqu6st2-9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-C4oMuPdfpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data = X_train\n",
        "label_data = y_train\n",
        "word_index = t.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWYfuRgRdoVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from keras.layers import Flatten, Dropout\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxDrjCR39gs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터셋 전처리 : 문장 길이 맞추기\n",
        "input_train = sequence.pad_sequences(input_train, maxlen=max_len)\n",
        "input_val = sequence.pad_sequences(input_val, maxlen=max_len)\n",
        "input_test = sequence.pad_sequences(input_test, maxlen=max_len)\n",
        "\n",
        "\n",
        "# one-hot 인코딩\n",
        "label_train = np_utils.to_categorical(label_train)\n",
        "label_val = np_utils.to_categorical(label_val)\n",
        "label_test = np_utils.to_categorical(label_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwnVzg2Mdp38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 테스트 파일일 경우에만 실행 !\n",
        "# 데이터셋 전처리 : 문장 길이 맞추기\n",
        "input_data = sequence.pad_sequences(input_data, maxlen=max_len)\n",
        "\n",
        "# one-hot 인코딩\n",
        "label_data = np_utils.to_categorical(label_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9nHAyOMErED",
        "colab_type": "text"
      },
      "source": [
        "# 사전 훈련된 워드 임베딩(Pre-trained Word Embedding)\n",
        "이번엔 케라스의 임베딩 층(embedding layer)과 사전 훈련된 워드 임베딩(pre-trained word embedding)을 가져와서 사용하는 것을 비교해봅니다. 자연어 처리를 하려고 할 때 갖고 있는 훈련 데이터의 단어들을 임베딩 층(embedding layer)을 구현하여 임베딩 벡터로 학습하는 경우가 있습니다. 케라스에서는 이를 Embedding()이라는 도구를 사용하여 구현합니다.\n",
        "\n",
        "그런데 위키피디아 등과 같은 방대한 코퍼스를 가지고 Word2vec, FastText, GloVe 등을 통해서 이미 미리 훈련된 임베딩 벡터를 불러오는 방법을 사용하는 경우도 있습니다. 이는 현재 갖고 있는 훈련 데이터를 임베딩 층으로 처음부터 학습을 하는 방법과는 대조됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUfomKeUcz1a",
        "colab_type": "text"
      },
      "source": [
        "## import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22SYkogCEz32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 현재 위치에 구글의 사전 훈련된 Word2Vec을 다운로드\n",
        "!wget \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNJQE-F8E0pJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 구글의 사전 훈련된 Word2vec 모델을 로드합니다.\n",
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftuXjqmaE3Vl",
        "colab_type": "text"
      },
      "source": [
        "구글의 사전 훈련된 Word2Vec 모델을 로드하여 word2vec_model에 저장합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lskbj7FEE3-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(word2vec_model.vectors.shape) # 모델의 크기 확인"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YM9GfILFF0F",
        "colab_type": "text"
      },
      "source": [
        "300의 차원을 가진 Word2Vec 벡터가 3,000,000개 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekdV3cy4FDuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 300)) #vocab_size는 맨 앞에서 데이터를 불러와 토크나이징 후 vocab_size를 추출한 값입니다. 안했다면 확인 후 실행시켜와야합니다.\n",
        "# 단어 집합 크기의 행과 300개의 열을 가지는 행렬 생성. 값은 전부 0으로 채워진다.\n",
        "np.shape(embedding_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6jeroXvFTIb",
        "colab_type": "text"
      },
      "source": [
        "모든 값이 0으로 채워진 임베딩 행렬을 만들어줍니다. 이번 문제의 단어는 총 16개이므로, 16 × 300의 크기를 가진 행렬을 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJWjke3hFVeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vector(word):\n",
        "    if word in word2vec_model:\n",
        "        return word2vec_model[word]\n",
        "    else:\n",
        "        return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNW5URI2FXHH",
        "colab_type": "text"
      },
      "source": [
        "word2vec_model에서 특정 단어를 입력하면 해당 단어의 임베딩 벡터를 리턴받을텐데, 만약 word2vec_model에 특정 단어의 임베딩 벡터가 없다면 None을 리턴하도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTPWCTDtFYzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word, i in t.word_index.items(): # 훈련 데이터의 단어 집합에서 단어와 정수 인덱스를 1개씩 꺼내온다.\n",
        "    temp = get_vector(word) # 단어(key) 해당되는 임베딩 벡터의 300개의 값(value)를 임시 변수에 저장\n",
        "    if temp is not None: # 만약 None이 아니라면 임베딩 벡터의 값을 리턴받은 것이므로\n",
        "        embedding_matrix[i] = temp # 해당 단어 위치의 행에 벡터의 값을 저장한다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg2xmS9-FcHv",
        "colab_type": "text"
      },
      "source": [
        "단어 집합으로부터 단어를 1개씩 호출하여 word2vec_model에 해당 단어의 임베딩 벡터값이 존재하는지 확인합니다. 만약 None이 아니라면 존재한다는 의미이므로 임베딩 행렬에 해당 단어의 인덱스 위치의 행에 임베딩 벡터의 값을 저장합니다. 이렇게 되면 현재 풀고자하는 문제의 16개의 단어와 맵핑되는 임베딩 행렬이 완성됩니다.\n",
        "\n",
        "제대로 맵핑이 됐는지 확인해볼까요? 기존에 word2vec_model에 저장되어 있던 단어 'nice'의 임베딩 벡터값을 확인해봅시다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pgci0-hFdiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(word2vec_model['olive'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj936fPwFf9N",
        "colab_type": "text"
      },
      "source": [
        "이 단어 'nice'는 현재 단어 집합에서 몇 번 인덱스를 가지는지 확인해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nMZ8AYvFg_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('단어 olive의 정수 인덱스 :', t.word_index['olive'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzOJjuVdFiyj",
        "colab_type": "text"
      },
      "source": [
        "1의 값을 가지므로 embedding_matirx의 1번 인덱스에는 단어 'nice'의 임베딩 벡터값이 있어야 합니다. 한 번 출력해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_dGNCxiFkzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(embedding_matrix[177])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZChKznqFmRB",
        "colab_type": "text"
      },
      "source": [
        "값이 word2vec_model에서 확인했던 것과 동일한 것을 확인할 수 있습니다. 단어 집합에 있는 다른 단어들에 대해서도 확인해보세요. 이제 Embedding에 사전 훈련된 embedding_matrix를 입력으로 넣어주고 모델을 학습시켜보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUF9MZRfHxMB",
        "colab_type": "text"
      },
      "source": [
        "### 모델 생성\n",
        "훈련할 때는 모든 코드를 실행하고, 테스트할 때는 아래 코드를 실행하면 안됩니다. \n",
        "위에서 불러온 테스트 데이터셋에는 라벨이 모두 없을 뿐더러(위에서 모두 0으로 변경해서 벡터화시켰습니다), 이미 훈련된 모델을 망가뜨리기 때문입니다. 라벨 값이 없기 때문에 model.fit도 불가능할겁니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTUDAzx5JBLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR5011pqH0a0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 특정 클래스에 대한 정밀도\n",
        "def single_class_precision(interesting_class_id):\n",
        "    def prec(y_true, y_pred):\n",
        "        class_id_true = K.argmax(y_true, axis=-1) # y_true: 실제 값, 티아노 및 텐스플로우의 텐서(tensor)\n",
        "        class_id_pred = K.argmax(y_pred, axis=-1) # y_pred: 예측 값, 티아노 및 텐스플로우의 텐서(tensor)\n",
        "        precision_mask = K.cast(K.equal(class_id_pred, interesting_class_id), 'int32')\n",
        "        class_prec_tensor = K.cast(K.equal(class_id_true, class_id_pred), 'int32') * precision_mask\n",
        "        class_prec = K.cast(K.sum(class_prec_tensor), 'float32') / K.cast(K.maximum(K.sum(precision_mask), 1), 'float32')\n",
        "        return class_prec\n",
        "    return prec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81gZIjF9H2O_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 특정 클래스에 대한 재현율\n",
        "def single_class_recall(interesting_class_id):\n",
        "    def recall(y_true, y_pred):\n",
        "        class_id_true = K.argmax(y_true, axis=-1)\n",
        "        class_id_pred = K.argmax(y_pred, axis=-1)\n",
        "        recall_mask = K.cast(K.equal(class_id_true, interesting_class_id), 'int32')\n",
        "        class_recall_tensor = K.cast(K.equal(class_id_true, class_id_pred), 'int32') * recall_mask\n",
        "        class_recall = K.cast(K.sum(class_recall_tensor), 'float32') / K.cast(K.maximum(K.sum(recall_mask), 1), 'float32')\n",
        "        return class_recall\n",
        "    return recall"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwykUhFwIJw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from keras.layers import Flatten, Dropout\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2jPSHLkH4TG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_len, trainable=True)) # 배치사이즈가 기존(128)보다 2배 커지면 어떻게 될까? => \n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(256,\n",
        "                 3,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(20, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8oL-6KFISmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF-kQpYgIVVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
        "              metrics=['accuracy',\n",
        "                       single_class_precision(0), single_class_recall(0),\n",
        "                       single_class_precision(1), single_class_recall(1),\n",
        "                       single_class_precision(2), single_class_recall(2),\n",
        "                       single_class_precision(3), single_class_recall(3),\n",
        "                       single_class_precision(4), single_class_recall(4),\n",
        "                       single_class_precision(5), single_class_recall(5),\n",
        "                       single_class_precision(6), single_class_recall(6),\n",
        "                       single_class_precision(7), single_class_recall(7),\n",
        "                       single_class_precision(8), single_class_recall(8),\n",
        "                       single_class_precision(9), single_class_recall(9),\n",
        "                       single_class_precision(10), single_class_recall(10),\n",
        "                       single_class_precision(11), single_class_recall(11),\n",
        "                       single_class_precision(12), single_class_recall(12),\n",
        "                       single_class_precision(13), single_class_recall(13),\n",
        "                       single_class_precision(14), single_class_recall(14),\n",
        "                       single_class_precision(15), single_class_recall(15),\n",
        "                       single_class_precision(16), single_class_recall(16),\n",
        "                       single_class_precision(17), single_class_recall(17),\n",
        "                       single_class_precision(18), single_class_recall(18),\n",
        "                       single_class_precision(19), single_class_recall(19)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLONB2LjIYBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(input_train, label_train, batch_size=256 ,epochs=15, verbose=1, validation_data=(input_val, label_val)) # validation_data는 각 훈련마다 결과값을 도출할 측정 데이터를 의미한다. 따라서 가중치 업데이트는 되지 않는다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrICctu5IbIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6. 모델 평가하기\n",
        "loss_and_metrics = model.evaluate(input_test, label_test, batch_size=256)\n",
        "print('## evaluation loss and_metrics ##')\n",
        "print(loss_and_metrics)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15mj6ltogHLA",
        "colab_type": "text"
      },
      "source": [
        "### 훈련데이터셋에서 예측하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0syWQYlIfz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data['label'] = model.predict_classes(input_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zgz0hK8Iiw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install xlsxwriter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPdl1Up3IlUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataframe = pd.DataFrame(train_data)\n",
        "dataframe.to_excel(\"/content/drive/My Drive/text/Word2Vec_result(512).xlsx\", engine='xlsxwriter', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj6fXgEdgQ_p",
        "colab_type": "text"
      },
      "source": [
        "### 새로 불러온 테스트데이터셋에서 예측하기(라벨값 없음)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2NMxBIef-02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.predict_classes(input_data)\n",
        "len(model.predict_classes(input_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sETpK61QgVkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data['label'] = model.predict_classes(input_data)\n",
        "len(test_data['label'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYy_xpD7gjgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataframe = pd.DataFrame(test_data)\n",
        "dataframe.to_excel(\"/content/drive/My Drive/text/Word2Vec_result(test2).xlsx\", engine='xlsxwriter', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYKO7HQSkYOS",
        "colab_type": "text"
      },
      "source": [
        "### 결과 보고\n",
        "\n",
        "loss_and_metrics = model.evaluate(input_test, label_test, batch_size=256)\n",
        "- 모델 평가에서의 배치사이즈를 256, 512로 설정하여 정답레이블-예측레이블 오차를 측장하여 비교해봤으나, 둘 다 39371개 중 810를 잘못 예측하였습니다. \n",
        "- 배치 사이즈가 1일 경우, 128일 경우 정밀도와 재현율이 현저하게 떨어졌습니다.\n",
        "- 256, 512의 경우 완벽하게 정확하게 예측이 일치하는 것으로 보아, 과대적합이 발생하여 훈련데이터에 적응해버린 것이 아닌가 추측됩니다. \n",
        "-- 잘못 생각한 것이었습니다. 평가층에서 배치 사이즈를 변경한다고하여, 이미 훈련된 모델로 predict할 땐 전혀 영향을 끼치지 않습니다. 단치 배치 사이즈에 따른 모델을 평가하는 인자인 것 같습니다. \n",
        "-- 따라서 평가에서의 배치 사이즈가 아닌 glove 임베딩 층의 차원을 변경하여 어떤 결과가 나오는지 확인해보곘습니다.\n",
        "- 일반적인 자체 데이터셋으로 임베딩층을 설정했을 때와, Pre-Trained GloVe 임베딩 층을 사용했을 때 결과가 별반 다르지 않았습니다."
      ]
    }
  ]
}